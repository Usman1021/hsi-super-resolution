{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8d36ef-2dff-4215-b798-08f878057a3d",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Model for HyperspectralvImage Super-resolution with Gradient-Aware Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67e0d5a-f4e5-472a-a2f2-e0c16e43f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:41:45.451344: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 15:41:45.467037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739886105.483079 1138738 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739886105.487987 1138738 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 15:41:45.504834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from spectral import open_image\n",
    "from scipy.io import loadmat \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, Activation, Add, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f646b955-67fe-4c08-8287-fc146607c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in loaded .mat file: dict_keys(['__header__', '__version__', '__globals__', 'pavia'])\n",
      "Hyperspectral image shape: (1096, 715, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739886110.649817 1138738 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_hr shape: (490, 144, 144, 6)\n",
      "X_validation_hr shape: (70, 144, 144, 6)\n",
      "X_test_hr shape: (140, 144, 144, 6)\n",
      "X_train_lr shape: (490, 72, 72, 6)\n",
      "X_validation_lr shape: (70, 72, 72, 6)\n",
      "X_test_lr shape: (140, 72, 72, 6)\n"
     ]
    }
   ],
   "source": [
    "# Set all seeds for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Ensure TensorFlow uses deterministic operations\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load the Pavia dataset\n",
    "try:\n",
    "    data = loadmat(\"Pavia.mat\")  # Ensure that the file path is correct\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading .mat file: {e}\")\n",
    "\n",
    "# Access the hyperspectral image using the correct key 'pavia'\n",
    "print(\"Keys in loaded .mat file:\", data.keys())\n",
    "if 'pavia' in data:\n",
    "    hyperspectral_image = data['pavia']\n",
    "else:\n",
    "    raise KeyError(\"'pavia' not found in the .mat file.\")\n",
    "\n",
    "# Check the shape of the hyperspectral image\n",
    "print(\"Hyperspectral image shape:\", hyperspectral_image.shape)\n",
    "\n",
    "# Convert to float32 for TensorFlow operations\n",
    "hyperspectral_image = hyperspectral_image.astype(np.float32)\n",
    "\n",
    "# Load the hyperspectral data using the spectral library\n",
    "data = hyperspectral_image  # Use the loaded hyperspectral image directly\n",
    "\n",
    "# Parameters\n",
    "patch_size = (144, 144)  # Size of patches to extract\n",
    "test_size = 0.2  # Proportion of data for testing\n",
    "validation_size = 0.1  # Proportion of data for validation\n",
    "downscale_factor = 2  # Factor to downscale patches\n",
    "nodata_value = -1  # Value that indicates \"no data\"\n",
    "group_size = 6  # Group size for spectral bands\n",
    "overlap_size = 2  # Overlap size for grouped bands\n",
    "\n",
    "# Function to group bands into overlapping subgroups\n",
    "def group_bands_with_overlap(data, group_size=6, overlap_size=2):\n",
    "    height, width, bands = data.shape\n",
    "    step_size = group_size - overlap_size  # Calculate step size based on overlap\n",
    "    grouped_data = []\n",
    "\n",
    "    # Create overlapping groups of bands\n",
    "    for g in range(0, bands - group_size + 1, step_size):\n",
    "        group = data[:, :, g:g + group_size]\n",
    "        grouped_data.append(group)\n",
    "    \n",
    "    return np.array(grouped_data)\n",
    "\n",
    "# Extract and downscale patches from hyperspectral data\n",
    "def extract_and_downscale_patches(data, patch_size, downscale_factor, nodata_value=0):\n",
    "    patches_hr = []\n",
    "    patches_lr = []\n",
    "    height, width, bands = data.shape\n",
    "\n",
    "    for i in range(0, height - patch_size[0] + 1, patch_size[0]):\n",
    "        for j in range(0, width - patch_size[1] + 1, patch_size[1]):\n",
    "            patch_hr = data[i:i + patch_size[0], j:j + patch_size[1], :]\n",
    "\n",
    "            # Check for nodata_value and skip patch extraction if present\n",
    "            if np.any(patch_hr == nodata_value):\n",
    "                continue\n",
    "            \n",
    "            patch_lr = tf.image.resize(patch_hr, \n",
    "                                       [patch_size[0] // downscale_factor, patch_size[1] // downscale_factor], \n",
    "                                       method='bilinear')\n",
    "            patches_hr.append(patch_hr)\n",
    "            patches_lr.append(patch_lr.numpy())  # Convert tensor to numpy\n",
    "\n",
    "    return np.array(patches_hr), np.array(patches_lr)\n",
    "\n",
    "# Group bands into overlapping subgroups\n",
    "grouped_data = group_bands_with_overlap(hyperspectral_image, group_size=group_size, overlap_size=overlap_size)\n",
    "\n",
    "# Extract and downscale patches for all groups\n",
    "all_patches_hr = []\n",
    "all_patches_lr = []\n",
    "\n",
    "for group in grouped_data:\n",
    "    patches_hr, patches_lr = extract_and_downscale_patches(group, patch_size, downscale_factor, nodata_value=nodata_value)\n",
    "    all_patches_hr.append(patches_hr)\n",
    "    all_patches_lr.append(patches_lr)\n",
    "\n",
    "# Convert lists to numpy arrays before shuffling\n",
    "all_patches_hr = np.array(all_patches_hr)\n",
    "all_patches_lr = np.array(all_patches_lr)\n",
    "\n",
    "# Concatenate patches from all groups\n",
    "all_patches_hr = np.concatenate(all_patches_hr, axis=0)\n",
    "all_patches_lr = np.concatenate(all_patches_lr, axis=0)\n",
    "\n",
    "# Calculate the number of patches\n",
    "num_patches = len(all_patches_hr)\n",
    "\n",
    "# Calculate sizes for training, validation, and testing sets\n",
    "train_size = int((1 - test_size - validation_size) * num_patches)\n",
    "validation_size = int(validation_size * num_patches)\n",
    "test_size = num_patches - (train_size + validation_size)  # Explicit calculation of test size\n",
    "\n",
    "# Shuffle indices for splitting the data\n",
    "indices = np.arange(num_patches)\n",
    "np.random.shuffle(indices)\n",
    "all_patches_hr = all_patches_hr[indices]\n",
    "all_patches_lr = all_patches_lr[indices]\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "X_train_hr, X_validation_hr, X_test_hr = np.split(all_patches_hr, [train_size, train_size + validation_size])\n",
    "X_train_lr, X_validation_lr, X_test_lr = np.split(all_patches_lr, [train_size, train_size + validation_size])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X_train_hr shape:\", X_train_hr.shape)\n",
    "print(\"X_validation_hr shape:\", X_validation_hr.shape)\n",
    "print(\"X_test_hr shape:\", X_test_hr.shape)\n",
    "\n",
    "print(\"X_train_lr shape:\", X_train_lr.shape)\n",
    "print(\"X_validation_lr shape:\", X_validation_lr.shape)\n",
    "print(\"X_test_lr shape:\", X_test_lr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38a918-f8fa-4ee6-9d67-0512babe7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spatial–Spectral Gradient Loss Function\n",
    "def spatial_spectral_gradient_loss(y_true, y_pred):\n",
    "    mse_loss = K.mean(K.square(y_true - y_pred))\n",
    "    \n",
    "    def spatial_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "        grad_y_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
    "        grad_y_true_y = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "        grad_y_pred_y = y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]\n",
    "        \n",
    "        loss_x = K.mean(K.square(grad_y_true_x - grad_y_pred_x))\n",
    "        loss_y = K.mean(K.square(grad_y_true_y - grad_y_pred_y))\n",
    "        \n",
    "        return loss_x + loss_y\n",
    "\n",
    "    def spectral_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_spectral = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
    "        grad_y_pred_spectral = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
    "        return K.mean(K.square(grad_y_true_spectral - grad_y_pred_spectral))\n",
    "\n",
    "    spatial_loss = spatial_gradient_loss(y_true, y_pred)\n",
    "    spectral_loss = spectral_gradient_loss(y_true, y_pred)\n",
    "    \n",
    "    total_loss = mse_loss + 0.1 * spatial_loss + 0.1 * spectral_loss\n",
    "    return total_loss\n",
    "\n",
    "# Residual Block\n",
    "def residual_block(x, filters=32):\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
    "    res = BatchNormalization()(res)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Spectral–Spatial Block\n",
    "def spectral_spatial_block(x, filters=32):\n",
    "    spatial = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    spatial = BatchNormalization()(spatial)\n",
    "    spatial = Activation('relu')(spatial)\n",
    "    \n",
    "    spectral = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    spectral = BatchNormalization()(spectral)\n",
    "    spectral = Activation('relu')(spectral)\n",
    "    \n",
    "    spatial = Conv2D(filters, (1, 1), padding='same')(spatial)\n",
    "    \n",
    "    combined = Concatenate(axis=-1)([spatial, spectral])\n",
    "    return combined\n",
    "\n",
    "# Spectral Unmixing Block\n",
    "def spectral_unmixing_block(x, num_endmembers=10):\n",
    "    x = Conv2D(num_endmembers, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return x\n",
    "\n",
    "# General Upsampling Block (Supports Transpose and Standard Upsampling)\n",
    "def upsample_block(x, filters, scale=2, use_transpose=True):\n",
    "    if use_transpose:\n",
    "        x = Conv2DTranspose(filters, (3, 3), strides=(scale, scale), padding='same')(x)\n",
    "    else:\n",
    "        x = UpSampling2D(size=(scale, scale))(x)\n",
    "        x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build Model with Configurable Upsampling\n",
    "def build_hybrid_sr_model(input_shape, num_endmembers=40, use_transpose=True):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (9, 9), padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(50):\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    x = spectral_spatial_block(x)\n",
    "    \n",
    "    x_unmixed = spectral_unmixing_block(x, num_endmembers)\n",
    "    \n",
    "    x_concat = Concatenate(axis=-1)([x, x_unmixed])\n",
    "    \n",
    "    x_up = upsample_block(x_concat, filters=64, scale=2, use_transpose=use_transpose)  # 2x upscaling\n",
    "  #  x_up = upsample_block(x_up, filters=32, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    " #   x_up = upsample_block(x_up, filters=16, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    "    \n",
    "    x_out = Conv2D(input_shape[-1], (3, 3), padding='same')(x_up)\n",
    "    x_out = Activation('linear')(x_out)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x_out)\n",
    "    return model\n",
    "\n",
    "# Define input shape and build model\n",
    "input_shape = (72, 72, 6)\n",
    "num_endmembers = 40\n",
    "use_transpose = True  # Set to False if you want to use UpSampling2D instead\n",
    "\n",
    "hybrid_sr_model = build_hybrid_sr_model(input_shape, num_endmembers=num_endmembers, use_transpose=use_transpose)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "hybrid_sr_model.compile(optimizer='adam', loss=spatial_spectral_gradient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecace29e-d0a5-47f8-9a82-5419221f0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:41:54.140425: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739886127.976544 1138818 service.cc:148] XLA service 0x7fb2e0021b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739886127.976565 1138818 service.cc:156]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2025-02-18 15:42:08.397850: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1739886130.080676 1138818 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  8/123\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 2373485.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739886135.639946 1138818 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 2035582.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:42:24.668739: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - loss: 2034968.0000 - val_loss: 1978907.2500\n",
      "Epoch 2/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1829891.5000 - val_loss: 1714298.8750\n",
      "Epoch 3/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1548235.6250 - val_loss: 1436753.6250\n",
      "Epoch 4/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1223059.8750 - val_loss: 1066882.2500\n",
      "Epoch 5/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 900890.2500 - val_loss: 750492.6250\n",
      "Epoch 6/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 621488.1250 - val_loss: 555658.5625\n",
      "Epoch 7/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 413311.5625 - val_loss: 232016.9219\n",
      "Epoch 8/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 277059.8438 - val_loss: 241399.4062\n",
      "Epoch 9/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 195259.5000 - val_loss: 120601.3594\n",
      "Epoch 10/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 153524.8594 - val_loss: 96359.2266\n",
      "Epoch 11/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 132226.3281 - val_loss: 84688.7656\n",
      "Epoch 12/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 120372.1562 - val_loss: 101851.8125\n",
      "Epoch 13/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 115205.5938 - val_loss: 89388.0703\n",
      "Epoch 14/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 104865.2891 - val_loss: 60674.5430\n",
      "Epoch 15/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 94712.9141 - val_loss: 53965.7852\n",
      "Epoch 16/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 90025.5078 - val_loss: 54014.5039\n",
      "Epoch 17/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 88613.6250 - val_loss: 94561.6172\n",
      "Epoch 18/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 85503.3750 - val_loss: 64907.8438\n",
      "Epoch 19/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 80907.7422 - val_loss: 63179.8945\n",
      "Epoch 20/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 76960.2109 - val_loss: 56873.4961\n",
      "Epoch 21/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 74482.2266 - val_loss: 51596.2305\n",
      "Epoch 22/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 71704.7422 - val_loss: 50835.7461\n",
      "Epoch 23/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 70279.8984 - val_loss: 51570.6016\n",
      "Epoch 24/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 69153.6094 - val_loss: 49645.7383\n",
      "Epoch 25/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 67825.6484 - val_loss: 48951.1836\n",
      "Epoch 26/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 66486.6172 - val_loss: 48742.0977\n",
      "Epoch 27/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 65374.1250 - val_loss: 51178.7305\n",
      "Epoch 28/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 64598.1719 - val_loss: 52846.6406\n",
      "Epoch 29/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 63756.3672 - val_loss: 49433.2148\n",
      "Epoch 30/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 62230.3359 - val_loss: 50826.4727\n",
      "Epoch 31/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 61693.8359 - val_loss: 52795.0938\n",
      "Epoch 32/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 61321.8125 - val_loss: 58793.7461\n",
      "Epoch 33/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 60043.2422 - val_loss: 57888.4414\n",
      "Epoch 34/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 60285.3828 - val_loss: 52648.2305\n",
      "Epoch 35/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 62405.3477 - val_loss: 74163.0312\n",
      "Epoch 36/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 63701.4102 - val_loss: 64327.1133\n",
      "Epoch 37/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 59221.3906 - val_loss: 65846.6250\n",
      "Epoch 38/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 59430.7305 - val_loss: 60757.1992\n",
      "Epoch 39/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 56617.7383 - val_loss: 118009.3203\n",
      "Epoch 40/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 60836.7617 - val_loss: 58056.1914\n",
      "Epoch 41/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 57946.1914 - val_loss: 55652.6797\n",
      "Epoch 42/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 57652.2969 - val_loss: 61362.4219\n",
      "Epoch 43/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 55480.7266 - val_loss: 87775.3984\n",
      "Epoch 44/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 55628.6797 - val_loss: 64937.1875\n",
      "Epoch 45/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 54506.8750 - val_loss: 64608.9414\n",
      "Epoch 46/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 52661.2305 - val_loss: 61747.4062\n",
      "Epoch 47/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 52750.0469 - val_loss: 74781.6641\n",
      "Epoch 48/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 52766.9922 - val_loss: 88577.5391\n",
      "Epoch 49/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 52167.9727 - val_loss: 84369.3828\n",
      "Epoch 50/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 53070.5273 - val_loss: 115275.7344\n",
      "Epoch 51/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 60477.0664 - val_loss: 54242.7148\n",
      "Epoch 52/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 52415.5039 - val_loss: 44894.3047\n",
      "Epoch 53/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 50595.5469 - val_loss: 46475.1406\n",
      "Epoch 54/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 48585.3711 - val_loss: 68664.1484\n",
      "Epoch 55/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 49577.4258 - val_loss: 44271.3633\n",
      "Epoch 56/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 47875.1133 - val_loss: 47946.6641\n",
      "Epoch 57/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 47078.5586 - val_loss: 46906.9805\n",
      "Epoch 58/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 47141.3281 - val_loss: 43430.2734\n",
      "Epoch 59/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46693.6953 - val_loss: 67005.3906\n",
      "Epoch 60/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46438.6562 - val_loss: 45725.3477\n",
      "Epoch 61/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46837.4570 - val_loss: 57954.1406\n",
      "Epoch 62/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46409.7734 - val_loss: 71408.6016\n",
      "Epoch 63/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 45312.5078 - val_loss: 72968.3516\n",
      "Epoch 64/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46074.2109 - val_loss: 54241.4219\n",
      "Epoch 65/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 45804.2930 - val_loss: 52973.8086\n",
      "Epoch 66/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 45993.5625 - val_loss: 63935.6289\n",
      "Epoch 67/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 45064.3086 - val_loss: 77415.1406\n",
      "Epoch 68/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 46272.3398 - val_loss: 66198.7109\n",
      "Epoch 69/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44840.0898 - val_loss: 71479.1719\n",
      "Epoch 70/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 45488.3984 - val_loss: 44904.4023\n",
      "Epoch 71/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44000.6445 - val_loss: 80841.2734\n",
      "Epoch 72/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 43612.7969 - val_loss: 58109.6758\n",
      "Epoch 73/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42576.5469 - val_loss: 55400.0195\n",
      "Epoch 74/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42437.4023 - val_loss: 44032.3477\n",
      "Epoch 75/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42564.5547 - val_loss: 57946.2227\n",
      "Epoch 76/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42432.1406 - val_loss: 90546.3750\n",
      "Epoch 77/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44337.1484 - val_loss: 48956.2695\n",
      "Epoch 78/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42537.3672 - val_loss: 62552.4727\n",
      "Epoch 79/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44158.5234 - val_loss: 52766.2930\n",
      "Epoch 80/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 44043.3086 - val_loss: 76115.3516\n",
      "Epoch 81/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 43557.0156 - val_loss: 49944.6680\n",
      "Epoch 82/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 43682.6133 - val_loss: 52192.1328\n",
      "Epoch 83/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 42787.1172 - val_loss: 50680.7930\n",
      "Epoch 84/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 41229.1992 - val_loss: 43475.8555\n",
      "Epoch 85/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 41721.2734 - val_loss: 44027.2773\n",
      "Epoch 86/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 40172.9531 - val_loss: 51214.2773\n",
      "Epoch 87/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 41547.0508 - val_loss: 39738.5625\n",
      "Epoch 88/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 40074.5977 - val_loss: 40646.1797\n",
      "Epoch 89/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 39885.7422 - val_loss: 39586.4688\n",
      "Epoch 90/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39476.7734 - val_loss: 40609.9844\n",
      "Epoch 91/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39230.4453 - val_loss: 39787.1406\n",
      "Epoch 92/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38647.2109 - val_loss: 40275.8516\n",
      "Epoch 93/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38658.7109 - val_loss: 44191.1680\n",
      "Epoch 94/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39708.4844 - val_loss: 64044.4062\n",
      "Epoch 95/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 41426.4453 - val_loss: 83371.5156\n",
      "Epoch 96/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 47248.8594 - val_loss: 57933.5117\n",
      "Epoch 97/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 41063.3555 - val_loss: 40527.1445\n",
      "Epoch 98/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38926.6641 - val_loss: 43871.5078\n",
      "Epoch 99/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38615.0117 - val_loss: 42836.6602\n",
      "Epoch 100/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 40270.4531 - val_loss: 39451.9922\n",
      "Epoch 101/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 38059.8555 - val_loss: 39093.9648\n",
      "Epoch 102/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38959.1641 - val_loss: 39615.6406\n",
      "Epoch 103/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 38789.2695 - val_loss: 38731.2148\n",
      "Epoch 104/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 37639.7617 - val_loss: 41395.1875\n",
      "Epoch 105/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38442.6328 - val_loss: 39875.8711\n",
      "Epoch 106/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 37809.8242 - val_loss: 38715.0078\n",
      "Epoch 107/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 37359.4141 - val_loss: 60223.7656\n",
      "Epoch 108/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38988.1328 - val_loss: 52002.9766\n",
      "Epoch 109/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39901.4922 - val_loss: 42315.6875\n",
      "Epoch 110/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38988.4648 - val_loss: 49225.0781\n",
      "Epoch 111/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 40136.8008 - val_loss: 39522.0000\n",
      "Epoch 112/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 41892.7578 - val_loss: 54780.8555\n",
      "Epoch 113/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 37718.6484 - val_loss: 38359.6055\n",
      "Epoch 114/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 36335.3594 - val_loss: 39863.1328\n",
      "Epoch 115/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 36277.8477 - val_loss: 40134.1641\n",
      "Epoch 116/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 37457.6367 - val_loss: 37452.6797\n",
      "Epoch 117/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35916.4727 - val_loss: 41610.3320\n",
      "Epoch 118/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 36004.7969 - val_loss: 51061.5391\n",
      "Epoch 119/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38742.7070 - val_loss: 50722.2695\n",
      "Epoch 120/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38509.6641 - val_loss: 40195.3008\n",
      "Epoch 121/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39865.0781 - val_loss: 58546.8594\n",
      "Epoch 122/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 37294.2070 - val_loss: 42627.8125\n",
      "Epoch 123/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35988.8945 - val_loss: 102395.1953\n",
      "Epoch 124/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 40352.3750 - val_loss: 39619.7539\n",
      "Epoch 125/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 37180.9570 - val_loss: 38008.6797\n",
      "Epoch 126/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 36269.8789 - val_loss: 45258.4531\n",
      "Epoch 127/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35321.3398 - val_loss: 39793.4805\n",
      "Epoch 128/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35696.9570 - val_loss: 40094.5195\n",
      "Epoch 129/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34149.9414 - val_loss: 43917.5234\n",
      "Epoch 130/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 33966.6641 - val_loss: 36754.4414\n",
      "Epoch 131/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34224.5781 - val_loss: 52792.1641\n",
      "Epoch 132/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34545.6250 - val_loss: 44854.5078\n",
      "Epoch 133/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35059.1641 - val_loss: 70765.4688\n",
      "Epoch 134/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 39305.5156 - val_loss: 37965.4648\n",
      "Epoch 135/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35906.4375 - val_loss: 46011.8086\n",
      "Epoch 136/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33721.5156 - val_loss: 36998.1484\n",
      "Epoch 137/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33197.9219 - val_loss: 43677.3906\n",
      "Epoch 138/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 32690.2324 - val_loss: 36030.1445\n",
      "Epoch 139/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 32518.4883 - val_loss: 37249.2461\n",
      "Epoch 140/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33364.3516 - val_loss: 65298.4922\n",
      "Epoch 141/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35275.6094 - val_loss: 135719.1406\n",
      "Epoch 142/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38330.0273 - val_loss: 51620.9805\n",
      "Epoch 143/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 38797.5781 - val_loss: 41944.4023\n",
      "Epoch 144/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33937.3555 - val_loss: 38892.3828\n",
      "Epoch 145/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35211.7969 - val_loss: 37015.5586\n",
      "Epoch 146/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 33420.0859 - val_loss: 35884.7695\n",
      "Epoch 147/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34535.2383 - val_loss: 54061.5078\n",
      "Epoch 148/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35065.4297 - val_loss: 50224.8047\n",
      "Epoch 149/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31642.8496 - val_loss: 58366.3203\n",
      "Epoch 150/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 35335.2461 - val_loss: 53596.2773\n",
      "Epoch 151/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33369.5898 - val_loss: 37717.5547\n",
      "Epoch 152/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 33527.9805 - val_loss: 74782.5781\n",
      "Epoch 153/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34260.8281 - val_loss: 55785.6641\n",
      "Epoch 154/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 32534.9160 - val_loss: 51884.0039\n",
      "Epoch 155/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34001.9141 - val_loss: 48509.5000\n",
      "Epoch 156/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31998.5625 - val_loss: 47544.8047\n",
      "Epoch 157/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31046.3066 - val_loss: 64119.9297\n",
      "Epoch 158/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31597.8164 - val_loss: 51869.5547\n",
      "Epoch 159/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30190.5430 - val_loss: 75269.9141\n",
      "Epoch 160/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 32104.6387 - val_loss: 42136.8945\n",
      "Epoch 161/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31054.6270 - val_loss: 73955.6875\n",
      "Epoch 162/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31056.4844 - val_loss: 44075.8945\n",
      "Epoch 163/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30317.4238 - val_loss: 63530.2930\n",
      "Epoch 164/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29275.6621 - val_loss: 43748.4922\n",
      "Epoch 165/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29772.7441 - val_loss: 39496.7500\n",
      "Epoch 166/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 31017.9512 - val_loss: 73098.1250\n",
      "Epoch 167/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 32133.4707 - val_loss: 42001.5195\n",
      "Epoch 168/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29742.6543 - val_loss: 65477.0000\n",
      "Epoch 169/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29185.3555 - val_loss: 43382.0078\n",
      "Epoch 170/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28939.7988 - val_loss: 59186.2422\n",
      "Epoch 171/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28534.0938 - val_loss: 53672.8828\n",
      "Epoch 172/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28948.2578 - val_loss: 54040.1445\n",
      "Epoch 173/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28705.6914 - val_loss: 38736.3086\n",
      "Epoch 174/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 29558.4844 - val_loss: 34367.6602\n",
      "Epoch 175/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 32251.0820 - val_loss: 53705.8789\n",
      "Epoch 176/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 34097.3086 - val_loss: 58539.6445\n",
      "Epoch 177/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30041.3281 - val_loss: 67389.4375\n",
      "Epoch 178/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30264.1367 - val_loss: 46898.0352\n",
      "Epoch 179/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28133.5371 - val_loss: 71619.4453\n",
      "Epoch 180/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28393.8477 - val_loss: 50400.9023\n",
      "Epoch 181/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27938.7402 - val_loss: 60406.1797\n",
      "Epoch 182/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27437.3477 - val_loss: 43829.7500\n",
      "Epoch 183/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27499.7129 - val_loss: 51944.1445\n",
      "Epoch 184/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 28755.8574 - val_loss: 34359.5703\n",
      "Epoch 185/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28571.1406 - val_loss: 82161.4062\n",
      "Epoch 186/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29403.4707 - val_loss: 35311.8672\n",
      "Epoch 187/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28765.4375 - val_loss: 45643.7969\n",
      "Epoch 188/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27796.7363 - val_loss: 34686.3594\n",
      "Epoch 189/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27392.5508 - val_loss: 47570.4766\n",
      "Epoch 190/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26895.1699 - val_loss: 46323.5977\n",
      "Epoch 191/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26682.6074 - val_loss: 30713.2031\n",
      "Epoch 192/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 27218.1621 - val_loss: 29807.4551\n",
      "Epoch 193/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28224.1211 - val_loss: 39789.2383\n",
      "Epoch 194/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26844.5000 - val_loss: 55880.8555\n",
      "Epoch 195/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27393.5469 - val_loss: 32818.5312\n",
      "Epoch 196/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27818.9141 - val_loss: 32429.8105\n",
      "Epoch 197/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26485.7051 - val_loss: 44198.6211\n",
      "Epoch 198/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26746.3730 - val_loss: 35009.3086\n",
      "Epoch 199/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26748.7109 - val_loss: 60356.9922\n",
      "Epoch 200/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 28322.2578 - val_loss: 59220.3438\n",
      "Epoch 201/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30855.0566 - val_loss: 33093.9414\n",
      "Epoch 202/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26695.5938 - val_loss: 34189.5625\n",
      "Epoch 203/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26334.1660 - val_loss: 43700.3984\n",
      "Epoch 204/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27710.9922 - val_loss: 67663.5938\n",
      "Epoch 205/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27982.8613 - val_loss: 73979.1484\n",
      "Epoch 206/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 30570.3867 - val_loss: 55987.3477\n",
      "Epoch 207/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26549.2715 - val_loss: 49201.9805\n",
      "Epoch 208/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25818.0059 - val_loss: 47947.5039\n",
      "Epoch 209/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25624.8340 - val_loss: 61361.4648\n",
      "Epoch 210/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25232.2520 - val_loss: 66485.7891\n",
      "Epoch 211/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26158.6934 - val_loss: 33805.5859\n",
      "Epoch 212/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26491.7031 - val_loss: 33402.0000\n",
      "Epoch 213/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 29164.7129 - val_loss: 39986.7500\n",
      "Epoch 214/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26437.5293 - val_loss: 58576.1445\n",
      "Epoch 215/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25264.9121 - val_loss: 43231.1562\n",
      "Epoch 216/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25470.2695 - val_loss: 64810.5938\n",
      "Epoch 217/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25444.6426 - val_loss: 50420.0117\n",
      "Epoch 218/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25224.8633 - val_loss: 35565.4297\n",
      "Epoch 219/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25295.6582 - val_loss: 37787.1445\n",
      "Epoch 220/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26338.7734 - val_loss: 55747.4961\n",
      "Epoch 221/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25997.7598 - val_loss: 43861.8711\n",
      "Epoch 222/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24903.7773 - val_loss: 41898.8203\n",
      "Epoch 223/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24086.3867 - val_loss: 47494.2305\n",
      "Epoch 224/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24882.7070 - val_loss: 33763.2422\n",
      "Epoch 225/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24415.4062 - val_loss: 50691.1367\n",
      "Epoch 226/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25056.4922 - val_loss: 53064.4141\n",
      "Epoch 227/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 26532.7031 - val_loss: 27673.4648\n",
      "Epoch 228/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25933.0664 - val_loss: 39243.3945\n",
      "Epoch 229/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25852.0117 - val_loss: 54074.3477\n",
      "Epoch 230/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25727.2168 - val_loss: 33414.3828\n",
      "Epoch 231/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24310.0742 - val_loss: 42412.3125\n",
      "Epoch 232/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23605.2734 - val_loss: 60631.2930\n",
      "Epoch 233/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24534.4844 - val_loss: 38655.9766\n",
      "Epoch 234/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24336.6445 - val_loss: 31251.6895\n",
      "Epoch 235/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23654.2695 - val_loss: 51443.4258\n",
      "Epoch 236/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23999.3145 - val_loss: 29612.9688\n",
      "Epoch 237/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25958.2344 - val_loss: 33872.2539\n",
      "Epoch 238/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24702.5488 - val_loss: 33597.3945\n",
      "Epoch 239/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25197.8867 - val_loss: 39958.8398\n",
      "Epoch 240/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24264.9512 - val_loss: 45077.7539\n",
      "Epoch 241/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23323.9062 - val_loss: 54008.3906\n",
      "Epoch 242/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23505.3750 - val_loss: 46451.2031\n",
      "Epoch 243/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23450.6797 - val_loss: 34318.7852\n",
      "Epoch 244/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23179.6250 - val_loss: 31572.7461\n",
      "Epoch 245/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23551.7266 - val_loss: 36912.9844\n",
      "Epoch 246/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23000.8887 - val_loss: 35685.1172\n",
      "Epoch 247/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24672.9258 - val_loss: 32106.1035\n",
      "Epoch 248/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 23492.3184 - val_loss: 27046.5020\n",
      "Epoch 249/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 24013.7930 - val_loss: 25651.6895\n",
      "Epoch 250/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23362.2637 - val_loss: 53595.5234\n",
      "Epoch 251/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25296.5723 - val_loss: 30779.7285\n",
      "Epoch 252/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23680.0371 - val_loss: 49301.8164\n",
      "Epoch 253/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24666.5117 - val_loss: 46194.4375\n",
      "Epoch 254/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24852.8340 - val_loss: 27409.4883\n",
      "Epoch 255/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23796.2168 - val_loss: 40197.5664\n",
      "Epoch 256/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22968.8613 - val_loss: 51310.3633\n",
      "Epoch 257/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23678.7266 - val_loss: 27643.2559\n",
      "Epoch 258/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24699.6992 - val_loss: 33863.4531\n",
      "Epoch 259/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23431.1562 - val_loss: 48293.6602\n",
      "Epoch 260/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24369.8750 - val_loss: 27938.2285\n",
      "Epoch 261/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23941.1211 - val_loss: 33908.1211\n",
      "Epoch 262/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23813.8008 - val_loss: 47662.7656\n",
      "Epoch 263/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23133.3672 - val_loss: 37127.6289\n",
      "Epoch 264/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22513.3379 - val_loss: 32615.0957\n",
      "Epoch 265/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22643.1641 - val_loss: 40057.3867\n",
      "Epoch 266/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22355.7422 - val_loss: 38761.2695\n",
      "Epoch 267/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22509.3496 - val_loss: 30677.6289\n",
      "Epoch 268/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22058.3125 - val_loss: 29172.0762\n",
      "Epoch 269/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22271.7715 - val_loss: 27418.2441\n",
      "Epoch 270/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22089.4980 - val_loss: 28629.6562\n",
      "Epoch 271/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21726.9062 - val_loss: 33978.0430\n",
      "Epoch 272/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21813.8047 - val_loss: 33290.8672\n",
      "Epoch 273/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23091.4316 - val_loss: 31584.7715\n",
      "Epoch 274/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24117.2188 - val_loss: 27277.0723\n",
      "Epoch 275/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22903.0898 - val_loss: 41882.8203\n",
      "Epoch 276/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22383.5039 - val_loss: 33575.6602\n",
      "Epoch 277/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21854.4023 - val_loss: 35284.4297\n",
      "Epoch 278/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22343.7480 - val_loss: 28303.7598\n",
      "Epoch 279/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21949.6895 - val_loss: 28064.4883\n",
      "Epoch 280/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21787.5625 - val_loss: 39772.0703\n",
      "Epoch 281/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22302.9199 - val_loss: 27851.7070\n",
      "Epoch 282/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23097.1738 - val_loss: 44607.9883\n",
      "Epoch 283/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 26456.0371 - val_loss: 40055.9766\n",
      "Epoch 284/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 25241.8750 - val_loss: 32813.6875\n",
      "Epoch 285/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22020.9336 - val_loss: 44207.0352\n",
      "Epoch 286/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22000.5508 - val_loss: 26202.4922\n",
      "Epoch 287/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 21718.3906 - val_loss: 25413.6777\n",
      "Epoch 288/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22491.7500 - val_loss: 31032.9707\n",
      "Epoch 289/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21606.6074 - val_loss: 30707.3574\n",
      "Epoch 290/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21228.6699 - val_loss: 32726.1211\n",
      "Epoch 291/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 21325.5645 - val_loss: 25004.8301\n",
      "Epoch 292/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21777.7559 - val_loss: 26430.8359\n",
      "Epoch 293/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21982.0410 - val_loss: 33233.4805\n",
      "Epoch 294/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21127.6484 - val_loss: 28376.9238\n",
      "Epoch 295/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21410.1094 - val_loss: 24077.5410\n",
      "Epoch 296/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21664.3008 - val_loss: 29153.4102\n",
      "Epoch 297/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21401.9395 - val_loss: 35171.8242\n",
      "Epoch 298/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21406.5391 - val_loss: 27012.5859\n",
      "Epoch 299/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21518.5352 - val_loss: 27576.7266\n",
      "Epoch 300/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21283.7578 - val_loss: 32553.1641\n",
      "Epoch 301/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20884.7832 - val_loss: 40746.8125\n",
      "Epoch 302/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21184.8594 - val_loss: 28328.5195\n",
      "Epoch 303/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21089.9297 - val_loss: 37377.2734\n",
      "Epoch 304/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21561.8535 - val_loss: 34197.2070\n",
      "Epoch 305/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21624.1523 - val_loss: 27606.5254\n",
      "Epoch 306/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20844.7402 - val_loss: 39984.0742\n",
      "Epoch 307/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20746.1992 - val_loss: 44692.6875\n",
      "Epoch 308/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20821.2969 - val_loss: 37319.3633\n",
      "Epoch 309/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20973.0371 - val_loss: 25914.9258\n",
      "Epoch 310/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21267.6270 - val_loss: 28875.8457\n",
      "Epoch 311/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21115.2793 - val_loss: 26607.4805\n",
      "Epoch 312/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21159.6094 - val_loss: 25707.6836\n",
      "Epoch 313/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21270.3613 - val_loss: 27392.5293\n",
      "Epoch 314/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21586.3242 - val_loss: 39044.8047\n",
      "Epoch 315/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21643.4688 - val_loss: 27971.5254\n",
      "Epoch 316/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21082.2402 - val_loss: 24903.2578\n",
      "Epoch 317/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20675.2695 - val_loss: 25757.1777\n",
      "Epoch 318/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22336.6172 - val_loss: 28822.8574\n",
      "Epoch 319/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22295.9805 - val_loss: 39018.2305\n",
      "Epoch 320/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21112.4062 - val_loss: 29338.8555\n",
      "Epoch 321/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20808.5996 - val_loss: 27507.4395\n",
      "Epoch 322/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20749.3105 - val_loss: 25723.3633\n",
      "Epoch 323/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21030.1777 - val_loss: 24204.7812\n",
      "Epoch 324/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20768.2344 - val_loss: 28918.1855\n",
      "Epoch 325/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21864.4570 - val_loss: 27940.7773\n",
      "Epoch 326/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21009.4805 - val_loss: 24872.1504\n",
      "Epoch 327/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20251.1328 - val_loss: 28515.1504\n",
      "Epoch 328/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20481.9844 - val_loss: 28934.7891\n",
      "Epoch 329/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20183.6230 - val_loss: 26822.8926\n",
      "Epoch 330/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20146.6484 - val_loss: 26217.7188\n",
      "Epoch 331/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20677.7461 - val_loss: 24776.0215\n",
      "Epoch 332/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20999.7949 - val_loss: 33285.3789\n",
      "Epoch 333/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20595.1094 - val_loss: 28858.4902\n",
      "Epoch 334/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21898.8906 - val_loss: 29700.2188\n",
      "Epoch 335/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20620.2969 - val_loss: 30730.2852\n",
      "Epoch 336/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 19755.1973 - val_loss: 23941.5293\n",
      "Epoch 337/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20320.2695 - val_loss: 28036.4453\n",
      "Epoch 338/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20884.9688 - val_loss: 44243.5664\n",
      "Epoch 339/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20874.5566 - val_loss: 28259.9922\n",
      "Epoch 340/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20322.2871 - val_loss: 25397.4434\n",
      "Epoch 341/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19313.1836 - val_loss: 26829.1074\n",
      "Epoch 342/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19309.1074 - val_loss: 32099.7852\n",
      "Epoch 343/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19497.2988 - val_loss: 26555.2598\n",
      "Epoch 344/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19605.0000 - val_loss: 24422.9023\n",
      "Epoch 345/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20731.0898 - val_loss: 53482.9219\n",
      "Epoch 346/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 27068.4531 - val_loss: 31466.3613\n",
      "Epoch 347/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 23580.1543 - val_loss: 36848.9375\n",
      "Epoch 348/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20157.6523 - val_loss: 27326.5469\n",
      "Epoch 349/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19775.4453 - val_loss: 29681.2188\n",
      "Epoch 350/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19516.3926 - val_loss: 25658.0859\n",
      "Epoch 351/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19648.9805 - val_loss: 26661.0957\n",
      "Epoch 352/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19448.3184 - val_loss: 24957.3184\n",
      "Epoch 353/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19584.6348 - val_loss: 26821.8574\n",
      "Epoch 354/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19375.3359 - val_loss: 27979.5098\n",
      "Epoch 355/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19218.1582 - val_loss: 26269.0723\n",
      "Epoch 356/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19029.4629 - val_loss: 24162.2188\n",
      "Epoch 357/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19285.9570 - val_loss: 27522.4297\n",
      "Epoch 358/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19377.9941 - val_loss: 25139.2695\n",
      "Epoch 359/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19702.1113 - val_loss: 34706.1602\n",
      "Epoch 360/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20154.2520 - val_loss: 28182.6895\n",
      "Epoch 361/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20669.8613 - val_loss: 29265.4648\n",
      "Epoch 362/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20697.7715 - val_loss: 28099.9824\n",
      "Epoch 363/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20206.9941 - val_loss: 30019.6934\n",
      "Epoch 364/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20018.5645 - val_loss: 31445.8926\n",
      "Epoch 365/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19389.2363 - val_loss: 36928.5039\n",
      "Epoch 366/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20567.3418 - val_loss: 104181.4609\n",
      "Epoch 367/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24604.7988 - val_loss: 31320.9141\n",
      "Epoch 368/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21466.8574 - val_loss: 37632.2656\n",
      "Epoch 369/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22188.5293 - val_loss: 38576.8594\n",
      "Epoch 370/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 22520.5332 - val_loss: 33060.2070\n",
      "Epoch 371/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20183.9375 - val_loss: 24950.0840\n",
      "Epoch 372/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19405.9473 - val_loss: 31475.3613\n",
      "Epoch 373/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18622.6934 - val_loss: 24386.6660\n",
      "Epoch 374/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19330.6387 - val_loss: 32623.5527\n",
      "Epoch 375/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18574.4609 - val_loss: 25592.7461\n",
      "Epoch 376/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19054.0156 - val_loss: 34435.5273\n",
      "Epoch 377/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 18357.6875 - val_loss: 23612.4473\n",
      "Epoch 378/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18312.9199 - val_loss: 28096.7227\n",
      "Epoch 379/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17944.9805 - val_loss: 28281.4219\n",
      "Epoch 380/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17964.0840 - val_loss: 24000.9492\n",
      "Epoch 381/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17886.5605 - val_loss: 24413.1367\n",
      "Epoch 382/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 17900.5195 - val_loss: 22359.9434\n",
      "Epoch 383/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17837.7285 - val_loss: 24087.9336\n",
      "Epoch 384/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18052.4570 - val_loss: 24641.4395\n",
      "Epoch 385/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20003.9551 - val_loss: 46769.1953\n",
      "Epoch 386/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 24237.0645 - val_loss: 29379.1152\n",
      "Epoch 387/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19101.7129 - val_loss: 29907.8477\n",
      "Epoch 388/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18824.5449 - val_loss: 22414.0508\n",
      "Epoch 389/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18800.2109 - val_loss: 22985.1309\n",
      "Epoch 390/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18114.5137 - val_loss: 46880.1172\n",
      "Epoch 391/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19251.6309 - val_loss: 54844.3867\n",
      "Epoch 392/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20127.5195 - val_loss: 24044.4668\n",
      "Epoch 393/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 18909.1211 - val_loss: 22348.9531\n",
      "Epoch 394/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18482.7129 - val_loss: 37229.5391\n",
      "Epoch 395/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18290.4766 - val_loss: 33800.7031\n",
      "Epoch 396/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18390.8027 - val_loss: 22646.7559\n",
      "Epoch 397/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18330.0293 - val_loss: 22958.2090\n",
      "Epoch 398/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 17761.8184 - val_loss: 21915.7051\n",
      "Epoch 399/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17764.9141 - val_loss: 25324.6719\n",
      "Epoch 400/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17818.0410 - val_loss: 30839.6172\n",
      "Epoch 401/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19359.5566 - val_loss: 23810.6016\n",
      "Epoch 402/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 18022.5547 - val_loss: 21691.9668\n",
      "Epoch 403/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18031.6621 - val_loss: 22312.8926\n",
      "Epoch 404/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18768.2754 - val_loss: 23562.8652\n",
      "Epoch 405/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18345.3242 - val_loss: 25145.8086\n",
      "Epoch 406/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18671.1328 - val_loss: 40353.8906\n",
      "Epoch 407/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18343.7363 - val_loss: 28143.3672\n",
      "Epoch 408/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 17505.2168 - val_loss: 21672.5527\n",
      "Epoch 409/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17364.3691 - val_loss: 23130.5781\n",
      "Epoch 410/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17220.5449 - val_loss: 25336.2129\n",
      "Epoch 411/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17363.9922 - val_loss: 25615.8652\n",
      "Epoch 412/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17253.6523 - val_loss: 23456.5488\n",
      "Epoch 413/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18930.5137 - val_loss: 24167.2402\n",
      "Epoch 414/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 20424.0059 - val_loss: 27180.1738\n",
      "Epoch 415/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18525.3027 - val_loss: 26510.9473\n",
      "Epoch 416/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17213.2148 - val_loss: 22315.0449\n",
      "Epoch 417/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17486.0625 - val_loss: 33435.5195\n",
      "Epoch 418/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19364.9492 - val_loss: 40645.6094\n",
      "Epoch 419/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18417.4609 - val_loss: 23801.4863\n",
      "Epoch 420/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17551.9043 - val_loss: 25217.5098\n",
      "Epoch 421/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17555.8398 - val_loss: 32903.2148\n",
      "Epoch 422/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17175.8301 - val_loss: 32443.0098\n",
      "Epoch 423/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17085.6680 - val_loss: 33156.7891\n",
      "Epoch 424/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16936.5195 - val_loss: 29502.0664\n",
      "Epoch 425/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16783.3945 - val_loss: 26204.9434\n",
      "Epoch 426/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17067.6348 - val_loss: 23446.8633\n",
      "Epoch 427/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17914.9766 - val_loss: 26994.7051\n",
      "Epoch 428/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18122.5977 - val_loss: 28823.0645\n",
      "Epoch 429/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17795.1191 - val_loss: 21964.1445\n",
      "Epoch 430/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17383.8242 - val_loss: 23427.4082\n",
      "Epoch 431/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16995.4219 - val_loss: 25297.5898\n",
      "Epoch 432/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17154.0527 - val_loss: 24611.4805\n",
      "Epoch 433/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17268.3887 - val_loss: 24016.0371\n",
      "Epoch 434/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17701.5898 - val_loss: 37009.9180\n",
      "Epoch 435/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18638.4277 - val_loss: 37517.2695\n",
      "Epoch 436/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17776.5332 - val_loss: 21997.2363\n",
      "Epoch 437/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17302.7383 - val_loss: 26273.2891\n",
      "Epoch 438/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17262.5977 - val_loss: 27412.0449\n",
      "Epoch 439/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16517.3613 - val_loss: 27973.5469\n",
      "Epoch 440/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16412.4336 - val_loss: 26620.8379\n",
      "Epoch 441/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16477.8477 - val_loss: 28382.5918\n",
      "Epoch 442/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16319.3369 - val_loss: 26876.0527\n",
      "Epoch 443/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16390.7637 - val_loss: 22603.2324\n",
      "Epoch 444/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17020.2578 - val_loss: 28220.3906\n",
      "Epoch 445/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19779.7539 - val_loss: 43632.1289\n",
      "Epoch 446/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21457.3398 - val_loss: 40340.0938\n",
      "Epoch 447/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17584.7852 - val_loss: 28653.4551\n",
      "Epoch 448/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17139.1387 - val_loss: 25037.5078\n",
      "Epoch 449/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16769.4688 - val_loss: 23442.2324\n",
      "Epoch 450/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16225.1055 - val_loss: 24083.1230\n",
      "Epoch 451/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16456.1699 - val_loss: 22134.0469\n",
      "Epoch 452/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16275.2764 - val_loss: 22575.5098\n",
      "Epoch 453/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16077.5547 - val_loss: 22569.8066\n",
      "Epoch 454/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16092.7188 - val_loss: 22376.1133\n",
      "Epoch 455/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16204.9541 - val_loss: 22081.9004\n",
      "Epoch 456/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 16722.7324 - val_loss: 21265.7773\n",
      "Epoch 457/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17518.7695 - val_loss: 27678.6250\n",
      "Epoch 458/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 19349.7539 - val_loss: 26664.6016\n",
      "Epoch 459/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18319.3750 - val_loss: 44039.2461\n",
      "Epoch 460/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21271.7246 - val_loss: 26093.0781\n",
      "Epoch 461/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21596.8320 - val_loss: 64268.1484\n",
      "Epoch 462/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 21566.4395 - val_loss: 27014.4102\n",
      "Epoch 463/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17272.1582 - val_loss: 29026.3262\n",
      "Epoch 464/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 16476.7402 - val_loss: 44175.6094\n",
      "Epoch 465/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16441.3887 - val_loss: 25505.4688\n",
      "Epoch 466/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16094.4453 - val_loss: 24000.8301\n",
      "Epoch 467/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15884.0498 - val_loss: 25016.4062\n",
      "Epoch 468/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15699.7764 - val_loss: 25261.6367\n",
      "Epoch 469/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15711.6738 - val_loss: 24740.9609\n",
      "Epoch 470/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15626.0342 - val_loss: 23770.9629\n",
      "Epoch 471/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15763.6582 - val_loss: 22444.4746\n",
      "Epoch 472/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16002.8975 - val_loss: 30171.3223\n",
      "Epoch 473/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16286.0234 - val_loss: 22306.6191\n",
      "Epoch 474/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16628.9922 - val_loss: 27764.7891\n",
      "Epoch 475/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17427.5918 - val_loss: 21645.2090\n",
      "Epoch 476/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17254.7344 - val_loss: 22687.5137\n",
      "Epoch 477/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17184.9062 - val_loss: 24384.6797\n",
      "Epoch 478/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17363.9863 - val_loss: 23089.3086\n",
      "Epoch 479/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16192.8945 - val_loss: 21685.9668\n",
      "Epoch 480/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15967.6338 - val_loss: 22207.1973\n",
      "Epoch 481/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15882.0625 - val_loss: 24623.6074\n",
      "Epoch 482/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15954.9199 - val_loss: 27900.8477\n",
      "Epoch 483/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17734.1543 - val_loss: 32241.6680\n",
      "Epoch 484/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 18554.1582 - val_loss: 25413.2207\n",
      "Epoch 485/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16653.3086 - val_loss: 23120.8965\n",
      "Epoch 486/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16405.2188 - val_loss: 35826.8398\n",
      "Epoch 487/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 17918.6230 - val_loss: 29128.2402\n",
      "Epoch 488/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16977.0195 - val_loss: 25907.1289\n",
      "Epoch 489/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16328.1748 - val_loss: 23997.9863\n",
      "Epoch 490/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16398.1289 - val_loss: 24005.8379\n",
      "Epoch 491/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16916.2031 - val_loss: 27088.2598\n",
      "Epoch 492/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16790.7227 - val_loss: 29978.4570\n",
      "Epoch 493/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16139.5596 - val_loss: 24909.3887\n",
      "Epoch 494/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15955.5010 - val_loss: 27280.1777\n",
      "Epoch 495/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16253.0020 - val_loss: 30884.4277\n",
      "Epoch 496/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15884.0293 - val_loss: 24310.5312\n",
      "Epoch 497/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16081.2461 - val_loss: 30614.6895\n",
      "Epoch 498/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16400.2402 - val_loss: 21985.1367\n",
      "Epoch 499/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16355.5742 - val_loss: 21353.6387\n",
      "Epoch 500/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16110.6709 - val_loss: 25607.2031\n",
      "Epoch 501/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15666.5166 - val_loss: 29076.7109\n",
      "Epoch 502/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15844.1123 - val_loss: 30283.1992\n",
      "Epoch 503/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15508.7217 - val_loss: 21958.2930\n",
      "Epoch 504/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 16619.9531 - val_loss: 25700.5566\n",
      "Epoch 505/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15804.7178 - val_loss: 22425.2402\n",
      "Epoch 506/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 15837.9424 - val_loss: 25175.7461\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### # Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the model and get the training history with early stopping\n",
    "history = hybrid_sr_model.fit(\n",
    "    X_train_lr, \n",
    "    X_train_hr, \n",
    "    epochs=1000, \n",
    "    batch_size=4, \n",
    "    validation_data=(X_validation_lr, X_validation_hr),\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")\n",
    "\n",
    "# Visualize training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f6bef1-abb5-4b52-bf37-31525ce1da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Average PSNR on the test set: 35.97617173194885\n",
      "Average SSIM on the test set: 0.9485125\n",
      "Average SAM on the test set (in degrees): 3.461847464662865\n",
      "Average Correlation Coefficient on the test set: 0.9812253984139788\n",
      "Average ERGAS on the test set: 5.859179152377739\n",
      "Average RMSE: 0.016512522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def psnr(y_true, y_pred, max_pixel=None):\n",
    "    \"\"\"\n",
    "    Compute PSNR for each spectral band separately and return the average.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth image, shape (H, W, B)\n",
    "        y_pred: Super-resolved image, shape (H, W, B)\n",
    "        max_pixel: Maximum pixel value (None = use actual max from y_true)\n",
    "    \n",
    "    Returns:\n",
    "        Average PSNR across all bands\n",
    "    \"\"\"\n",
    "    if max_pixel is None:\n",
    "        max_pixel = np.max(y_true)  # Auto-detect max value if not provided\n",
    "\n",
    "    B = y_true.shape[-1]  # Number of spectral bands\n",
    "    psnr_values = []\n",
    "    \n",
    "    for i in range(B):  # Loop over bands\n",
    "        mse = np.mean((y_true[..., i] - y_pred[..., i]) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr_values.append(float('inf'))  # Perfect reconstruction\n",
    "        else:\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "            psnr_values.append(psnr)\n",
    "    \n",
    "    return np.mean(psnr_values)  # Average across bands\n",
    "\n",
    "# Function to calculate SSIM with channel_axis\n",
    "def ssim_value(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape}\")\n",
    "    \n",
    "    data_range = y_true.max() - y_true.min()  # Calculate data range from y_true\n",
    "    ssim_val = ssim(y_true, y_pred, data_range=data_range, channel_axis=-1)\n",
    "    return ssim_val\n",
    "\n",
    "# Function to calculate Correlation Coefficient\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    corr_matrix = np.corrcoef(y_true_flat, y_pred_flat)\n",
    "    corr_value = corr_matrix[0, 1]\n",
    "    return corr_value\n",
    "\n",
    "# Function to calculate Spectral Angle Mapper (SAM) in degrees\n",
    "def sam(y_true, y_pred):\n",
    "    y_true_reshaped = y_true.reshape(-1, y_true.shape[-1])\n",
    "    y_pred_reshaped = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    non_zero_mask = (np.linalg.norm(y_true_reshaped, axis=1) > 1e-10) & (np.linalg.norm(y_pred_reshaped, axis=1) > 1e-10)\n",
    "    dot_product = np.sum(y_true_reshaped[non_zero_mask] * y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    norm_true = np.linalg.norm(y_true_reshaped[non_zero_mask], axis=1)\n",
    "    norm_pred = np.linalg.norm(y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        angles = np.arccos(np.clip(dot_product / (norm_true * norm_pred), -1.0, 1.0))\n",
    "    \n",
    "    if angles.size > 0:\n",
    "        sam_value_degrees = np.mean(angles) * (180 / np.pi)\n",
    "    else:\n",
    "        sam_value_degrees = 0\n",
    "    \n",
    "    return sam_value_degrees\n",
    "\n",
    "# Function to normalize the images\n",
    "def normalize(image):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE) for hyperspectral images (normalized)\n",
    "def rmse_bandwise(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch between true and predicted images.\")\n",
    "    \n",
    "    bands = y_true.shape[-1]\n",
    "    rmse_per_band = []\n",
    "\n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        \n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)\n",
    "        rmse_band_value = np.sqrt(mse_band)\n",
    "        rmse_per_band.append(rmse_band_value)\n",
    "\n",
    "    # Normalize RMSE by the maximum value in y_true across all bands\n",
    "    max_value = np.max(y_true)\n",
    "    normalized_rmse = np.mean(rmse_per_band) / max_value\n",
    "    return normalized_rmse\n",
    "\n",
    "# Function to calculate ERGAS\n",
    "def ergas(y_true, y_pred, scale):\n",
    "    bands = y_true.shape[-1]\n",
    "    ergas_value = 0\n",
    "    \n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        mean_band_true = np.mean(band_true)\n",
    "        \n",
    "        # Calculate RMSE for the band without using a separate function\n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)  # Mean Squared Error for the band\n",
    "        rmse_band = np.sqrt(mse_band)  # Root Mean Squared Error for the band\n",
    "        \n",
    "        ergas_value += (rmse_band / mean_band_true) ** 2\n",
    "    \n",
    "    ergas_value = 100 * (1 / scale) * np.sqrt(ergas_value / bands)\n",
    "    return ergas_value\n",
    "\n",
    "# Assuming hybrid_sr_model is trained, and X_test_lr, X_test_hr are defined\n",
    "predicted_hr_images = hybrid_sr_model.predict(X_test_lr, batch_size=4)\n",
    "\n",
    "downscale_factor = 2 # ERGAS downscale factor\n",
    "\n",
    "# Validate shapes match for test and predictions\n",
    "if predicted_hr_images.shape != X_test_hr.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_hr_images shape {predicted_hr_images.shape} vs X_test_hr shape {X_test_hr.shape}\")\n",
    "\n",
    "# Calculate metrics per test sample\n",
    "psnr_values, ssim_values, cc_values, sam_values, ergas_values, rmse_values = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test_hr)):\n",
    "    psnr_values.append(psnr(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ssim_values.append(ssim_value(X_test_hr[i], predicted_hr_images[i]))\n",
    "    cc_values.append(correlation_coefficient(X_test_hr[i], predicted_hr_images[i]))\n",
    "    sam_values.append(sam(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ergas_values.append(ergas(X_test_hr[i], predicted_hr_images[i], downscale_factor))\n",
    "    rmse_values.append(rmse_bandwise(X_test_hr[i], predicted_hr_images[i]))\n",
    "\n",
    "# Average metrics\n",
    "average_psnr = np.mean(psnr_values)\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_cc = np.mean(cc_values)\n",
    "average_sam = np.mean(sam_values)\n",
    "average_ergas = np.mean(ergas_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Average PSNR on the test set:\", average_psnr)\n",
    "print(\"Average SSIM on the test set:\", average_ssim)\n",
    "print(\"Average SAM on the test set (in degrees):\", average_sam)\n",
    "print(\"Average Correlation Coefficient on the test set:\", average_cc)\n",
    "print(\"Average ERGAS on the test set:\", average_ergas)\n",
    "print(\"Average RMSE:\", average_rmse)  # Indicate RMSE is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfb77-74c1-4c34-825b-e309932acfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8d36ef-2dff-4215-b798-08f878057a3d",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Model for HyperspectralvImage Super-resolution with Gradient-Aware Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67e0d5a-f4e5-472a-a2f2-e0c16e43f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:14:28.726679: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 15:14:28.741811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739884468.757553 1125108 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739884468.762445 1125108 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 15:14:28.779794: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from spectral import open_image\n",
    "from scipy.io import loadmat \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, Activation, Add, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f646b955-67fe-4c08-8287-fc146607c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in loaded .mat file: dict_keys(['__header__', '__version__', '__globals__', 'pavia'])\n",
      "Hyperspectral image shape: (1096, 715, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739884475.105701 1125108 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_hr shape: (490, 144, 144, 6)\n",
      "X_validation_hr shape: (70, 144, 144, 6)\n",
      "X_test_hr shape: (140, 144, 144, 6)\n",
      "X_train_lr shape: (490, 36, 36, 6)\n",
      "X_validation_lr shape: (70, 36, 36, 6)\n",
      "X_test_lr shape: (140, 36, 36, 6)\n"
     ]
    }
   ],
   "source": [
    "# Set all seeds for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Ensure TensorFlow uses deterministic operations\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load the Pavia dataset\n",
    "try:\n",
    "    data = loadmat(\"Pavia.mat\")  # Ensure that the file path is correct\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading .mat file: {e}\")\n",
    "\n",
    "# Access the hyperspectral image using the correct key 'pavia'\n",
    "print(\"Keys in loaded .mat file:\", data.keys())\n",
    "if 'pavia' in data:\n",
    "    hyperspectral_image = data['pavia']\n",
    "else:\n",
    "    raise KeyError(\"'pavia' not found in the .mat file.\")\n",
    "\n",
    "# Check the shape of the hyperspectral image\n",
    "print(\"Hyperspectral image shape:\", hyperspectral_image.shape)\n",
    "\n",
    "# Convert to float32 for TensorFlow operations\n",
    "hyperspectral_image = hyperspectral_image.astype(np.float32)\n",
    "\n",
    "# Load the hyperspectral data using the spectral library\n",
    "data = hyperspectral_image  # Use the loaded hyperspectral image directly\n",
    "\n",
    "# Parameters\n",
    "patch_size = (144, 144)  # Size of patches to extract\n",
    "test_size = 0.2  # Proportion of data for testing\n",
    "validation_size = 0.1  # Proportion of data for validation\n",
    "downscale_factor = 4  # Factor to downscale patches\n",
    "nodata_value = -1  # Value that indicates \"no data\"\n",
    "group_size = 6  # Group size for spectral bands\n",
    "overlap_size = 2  # Overlap size for grouped bands\n",
    "\n",
    "# Function to group bands into overlapping subgroups\n",
    "def group_bands_with_overlap(data, group_size=6, overlap_size=2):\n",
    "    height, width, bands = data.shape\n",
    "    step_size = group_size - overlap_size  # Calculate step size based on overlap\n",
    "    grouped_data = []\n",
    "\n",
    "    # Create overlapping groups of bands\n",
    "    for g in range(0, bands - group_size + 1, step_size):\n",
    "        group = data[:, :, g:g + group_size]\n",
    "        grouped_data.append(group)\n",
    "    \n",
    "    return np.array(grouped_data)\n",
    "\n",
    "# Extract and downscale patches from hyperspectral data\n",
    "def extract_and_downscale_patches(data, patch_size, downscale_factor, nodata_value=0):\n",
    "    patches_hr = []\n",
    "    patches_lr = []\n",
    "    height, width, bands = data.shape\n",
    "\n",
    "    for i in range(0, height - patch_size[0] + 1, patch_size[0]):\n",
    "        for j in range(0, width - patch_size[1] + 1, patch_size[1]):\n",
    "            patch_hr = data[i:i + patch_size[0], j:j + patch_size[1], :]\n",
    "\n",
    "            # Check for nodata_value and skip patch extraction if present\n",
    "            if np.any(patch_hr == nodata_value):\n",
    "                continue\n",
    "            \n",
    "            patch_lr = tf.image.resize(patch_hr, \n",
    "                                       [patch_size[0] // downscale_factor, patch_size[1] // downscale_factor], \n",
    "                                       method='bilinear')\n",
    "            patches_hr.append(patch_hr)\n",
    "            patches_lr.append(patch_lr.numpy())  # Convert tensor to numpy\n",
    "\n",
    "    return np.array(patches_hr), np.array(patches_lr)\n",
    "\n",
    "# Group bands into overlapping subgroups\n",
    "grouped_data = group_bands_with_overlap(hyperspectral_image, group_size=group_size, overlap_size=overlap_size)\n",
    "\n",
    "# Extract and downscale patches for all groups\n",
    "all_patches_hr = []\n",
    "all_patches_lr = []\n",
    "\n",
    "for group in grouped_data:\n",
    "    patches_hr, patches_lr = extract_and_downscale_patches(group, patch_size, downscale_factor, nodata_value=nodata_value)\n",
    "    all_patches_hr.append(patches_hr)\n",
    "    all_patches_lr.append(patches_lr)\n",
    "\n",
    "# Convert lists to numpy arrays before shuffling\n",
    "all_patches_hr = np.array(all_patches_hr)\n",
    "all_patches_lr = np.array(all_patches_lr)\n",
    "\n",
    "# Concatenate patches from all groups\n",
    "all_patches_hr = np.concatenate(all_patches_hr, axis=0)\n",
    "all_patches_lr = np.concatenate(all_patches_lr, axis=0)\n",
    "\n",
    "# Calculate the number of patches\n",
    "num_patches = len(all_patches_hr)\n",
    "\n",
    "# Calculate sizes for training, validation, and testing sets\n",
    "train_size = int((1 - test_size - validation_size) * num_patches)\n",
    "validation_size = int(validation_size * num_patches)\n",
    "test_size = num_patches - (train_size + validation_size)  # Explicit calculation of test size\n",
    "\n",
    "# Shuffle indices for splitting the data\n",
    "indices = np.arange(num_patches)\n",
    "np.random.shuffle(indices)\n",
    "all_patches_hr = all_patches_hr[indices]\n",
    "all_patches_lr = all_patches_lr[indices]\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "X_train_hr, X_validation_hr, X_test_hr = np.split(all_patches_hr, [train_size, train_size + validation_size])\n",
    "X_train_lr, X_validation_lr, X_test_lr = np.split(all_patches_lr, [train_size, train_size + validation_size])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X_train_hr shape:\", X_train_hr.shape)\n",
    "print(\"X_validation_hr shape:\", X_validation_hr.shape)\n",
    "print(\"X_test_hr shape:\", X_test_hr.shape)\n",
    "\n",
    "print(\"X_train_lr shape:\", X_train_lr.shape)\n",
    "print(\"X_validation_lr shape:\", X_validation_lr.shape)\n",
    "print(\"X_test_lr shape:\", X_test_lr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38a918-f8fa-4ee6-9d67-0512babe7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spatial–Spectral Gradient Loss Function\n",
    "def spatial_spectral_gradient_loss(y_true, y_pred):\n",
    "    mse_loss = K.mean(K.square(y_true - y_pred))\n",
    "    \n",
    "    def spatial_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "        grad_y_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
    "        grad_y_true_y = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "        grad_y_pred_y = y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]\n",
    "        \n",
    "        loss_x = K.mean(K.square(grad_y_true_x - grad_y_pred_x))\n",
    "        loss_y = K.mean(K.square(grad_y_true_y - grad_y_pred_y))\n",
    "        \n",
    "        return loss_x + loss_y\n",
    "\n",
    "    def spectral_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_spectral = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
    "        grad_y_pred_spectral = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
    "        return K.mean(K.square(grad_y_true_spectral - grad_y_pred_spectral))\n",
    "\n",
    "    spatial_loss = spatial_gradient_loss(y_true, y_pred)\n",
    "    spectral_loss = spectral_gradient_loss(y_true, y_pred)\n",
    "    \n",
    "    total_loss = mse_loss + 0.1 * spatial_loss + 0.1 * spectral_loss\n",
    "    return total_loss\n",
    "\n",
    "# Residual Block\n",
    "def residual_block(x, filters=32):\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
    "    res = BatchNormalization()(res)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Spectral–Spatial Block\n",
    "def spectral_spatial_block(x, filters=32):\n",
    "    spatial = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    spatial = BatchNormalization()(spatial)\n",
    "    spatial = Activation('relu')(spatial)\n",
    "    \n",
    "    spectral = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    spectral = BatchNormalization()(spectral)\n",
    "    spectral = Activation('relu')(spectral)\n",
    "    \n",
    "    spatial = Conv2D(filters, (1, 1), padding='same')(spatial)\n",
    "    \n",
    "    combined = Concatenate(axis=-1)([spatial, spectral])\n",
    "    return combined\n",
    "\n",
    "# Spectral Unmixing Block\n",
    "def spectral_unmixing_block(x, num_endmembers=10):\n",
    "    x = Conv2D(num_endmembers, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return x\n",
    "\n",
    "# General Upsampling Block (Supports Transpose and Standard Upsampling)\n",
    "def upsample_block(x, filters, scale=2, use_transpose=True):\n",
    "    if use_transpose:\n",
    "        x = Conv2DTranspose(filters, (3, 3), strides=(scale, scale), padding='same')(x)\n",
    "    else:\n",
    "        x = UpSampling2D(size=(scale, scale))(x)\n",
    "        x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build Model with Configurable Upsampling\n",
    "def build_hybrid_sr_model(input_shape, num_endmembers=40, use_transpose=True):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (9, 9), padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(16):\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    x = spectral_spatial_block(x)\n",
    "    \n",
    "    x_unmixed = spectral_unmixing_block(x, num_endmembers)\n",
    "    \n",
    "    x_concat = Concatenate(axis=-1)([x, x_unmixed])\n",
    "    \n",
    "    x_up = upsample_block(x_concat, filters=64, scale=2, use_transpose=use_transpose)  # 2x upscaling\n",
    "    x_up = upsample_block(x_up, filters=32, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    " #   x_up = upsample_block(x_up, filters=16, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    "    \n",
    "    x_out = Conv2D(input_shape[-1], (3, 3), padding='same')(x_up)\n",
    "    x_out = Activation('linear')(x_out)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x_out)\n",
    "    return model\n",
    "\n",
    "# Define input shape and build model\n",
    "input_shape = (36, 36, 6)\n",
    "num_endmembers = 40\n",
    "use_transpose = True  # Set to False if you want to use UpSampling2D instead\n",
    "\n",
    "hybrid_sr_model = build_hybrid_sr_model(input_shape, num_endmembers=num_endmembers, use_transpose=use_transpose)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "hybrid_sr_model.compile(optimizer='adam', loss=spatial_spectral_gradient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecace29e-d0a5-47f8-9a82-5419221f0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:14:39.503664: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739884494.285585 1125191 service.cc:148] XLA service 0x7f00f409d620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739884494.285611 1125191 service.cc:156]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2025-02-18 15:14:54.767508: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1739884496.712253 1125191 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 12/123\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2310043.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739884504.489057 1125191 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 2054162.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:15:14.120252: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 101ms/step - loss: 2053719.7500 - val_loss: 2152903.7500\n",
      "Epoch 2/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 1949039.2500 - val_loss: 1961895.7500\n",
      "Epoch 3/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1795188.6250 - val_loss: 1779107.7500\n",
      "Epoch 4/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1600372.7500 - val_loss: 1539267.8750\n",
      "Epoch 5/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1377871.0000 - val_loss: 1223375.3750\n",
      "Epoch 6/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1143037.3750 - val_loss: 985263.3125\n",
      "Epoch 7/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 918877.5000 - val_loss: 681162.8125\n",
      "Epoch 8/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 722625.6875 - val_loss: 534039.6250\n",
      "Epoch 9/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 561025.5000 - val_loss: 389855.5312\n",
      "Epoch 10/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 436014.2188 - val_loss: 291124.2812\n",
      "Epoch 11/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 346798.6250 - val_loss: 250194.9688\n",
      "Epoch 12/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 285456.2188 - val_loss: 232045.2344\n",
      "Epoch 13/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 246458.5000 - val_loss: 187987.2344\n",
      "Epoch 14/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 224555.2188 - val_loss: 229922.2188\n",
      "Epoch 15/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 212151.2344 - val_loss: 188594.0781\n",
      "Epoch 16/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 203549.4062 - val_loss: 207041.7500\n",
      "Epoch 17/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 194981.6875 - val_loss: 168130.4844\n",
      "Epoch 18/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 188330.9375 - val_loss: 177182.1875\n",
      "Epoch 19/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 178702.4531 - val_loss: 141445.9062\n",
      "Epoch 20/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 170189.5312 - val_loss: 140530.4375\n",
      "Epoch 21/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 166559.7031 - val_loss: 147447.3594\n",
      "Epoch 22/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 160182.6562 - val_loss: 139651.0938\n",
      "Epoch 23/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 157480.0000 - val_loss: 147001.5312\n",
      "Epoch 24/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 157796.0625 - val_loss: 165249.0000\n",
      "Epoch 25/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 155439.1719 - val_loss: 135198.9375\n",
      "Epoch 26/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 152293.2969 - val_loss: 150272.2500\n",
      "Epoch 27/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 150337.3438 - val_loss: 178733.1719\n",
      "Epoch 28/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 147859.3125 - val_loss: 164126.0625\n",
      "Epoch 29/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 145753.1562 - val_loss: 142708.6875\n",
      "Epoch 30/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 145800.3438 - val_loss: 154686.1562\n",
      "Epoch 31/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 147257.5156 - val_loss: 140578.6875\n",
      "Epoch 32/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 139212.8906 - val_loss: 153823.6094\n",
      "Epoch 33/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 143882.3438 - val_loss: 161060.3281\n",
      "Epoch 34/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 138876.0156 - val_loss: 167005.9688\n",
      "Epoch 35/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 134395.6562 - val_loss: 167264.7969\n",
      "Epoch 36/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 132367.5781 - val_loss: 169744.0469\n",
      "Epoch 37/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 130325.6406 - val_loss: 158512.8594\n",
      "Epoch 38/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 128588.7969 - val_loss: 135332.5625\n",
      "Epoch 39/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 128950.5000 - val_loss: 157718.9062\n",
      "Epoch 40/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 129353.7969 - val_loss: 170076.3281\n",
      "Epoch 41/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 126521.1328 - val_loss: 175264.7500\n",
      "Epoch 42/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 122509.2422 - val_loss: 151150.2500\n",
      "Epoch 43/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 124292.8047 - val_loss: 173286.2500\n",
      "Epoch 44/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 119854.2969 - val_loss: 175002.8281\n",
      "Epoch 45/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 118074.2344 - val_loss: 168504.5625\n",
      "Epoch 46/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 117813.6562 - val_loss: 186173.6719\n",
      "Epoch 47/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 115606.5312 - val_loss: 168126.8594\n",
      "Epoch 48/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 113819.4609 - val_loss: 156768.0469\n",
      "Epoch 49/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 114347.1953 - val_loss: 169799.3438\n",
      "Epoch 50/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 115345.7422 - val_loss: 161411.4688\n",
      "Epoch 51/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 123894.7266 - val_loss: 187529.6406\n",
      "Epoch 52/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 112809.8281 - val_loss: 149314.7812\n",
      "Epoch 53/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 109873.5859 - val_loss: 166731.1719\n",
      "Epoch 54/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 107378.3984 - val_loss: 139045.2656\n",
      "Epoch 55/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 109773.3594 - val_loss: 159912.4062\n",
      "Epoch 56/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 109498.3594 - val_loss: 148690.9844\n",
      "Epoch 57/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 104542.1875 - val_loss: 152837.7344\n",
      "Epoch 58/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 104280.0938 - val_loss: 148779.0156\n",
      "Epoch 59/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 102388.6641 - val_loss: 140923.1250\n",
      "Epoch 60/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 101821.5391 - val_loss: 164452.7188\n",
      "Epoch 61/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 102124.2266 - val_loss: 148664.9844\n",
      "Epoch 62/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 102830.3672 - val_loss: 141243.5938\n",
      "Epoch 63/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 100122.2812 - val_loss: 129311.5312\n",
      "Epoch 64/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 102922.1094 - val_loss: 136052.9062\n",
      "Epoch 65/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 99487.8906 - val_loss: 126567.3984\n",
      "Epoch 66/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 97025.7266 - val_loss: 139847.1250\n",
      "Epoch 67/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 97264.2109 - val_loss: 125864.4141\n",
      "Epoch 68/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 94891.9297 - val_loss: 128327.6406\n",
      "Epoch 69/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 94655.3516 - val_loss: 120679.0547\n",
      "Epoch 70/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 92411.4766 - val_loss: 127979.0312\n",
      "Epoch 71/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 92857.2891 - val_loss: 120974.3125\n",
      "Epoch 72/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 92499.5859 - val_loss: 143198.3906\n",
      "Epoch 73/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 94874.7344 - val_loss: 109757.4688\n",
      "Epoch 74/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 94701.3594 - val_loss: 119590.6797\n",
      "Epoch 75/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 93663.6953 - val_loss: 110500.1250\n",
      "Epoch 76/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 91894.8281 - val_loss: 112450.1250\n",
      "Epoch 77/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 95133.8594 - val_loss: 116656.5547\n",
      "Epoch 78/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 91810.7344 - val_loss: 128976.9297\n",
      "Epoch 79/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 87973.6641 - val_loss: 121180.8281\n",
      "Epoch 80/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 86473.9141 - val_loss: 126123.2578\n",
      "Epoch 81/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 86267.6484 - val_loss: 117707.5391\n",
      "Epoch 82/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 84768.4375 - val_loss: 117615.2891\n",
      "Epoch 83/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 84619.4922 - val_loss: 115261.0781\n",
      "Epoch 84/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 89060.5703 - val_loss: 107779.3750\n",
      "Epoch 85/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 89214.1797 - val_loss: 104436.2734\n",
      "Epoch 86/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 87354.6250 - val_loss: 113791.8125\n",
      "Epoch 87/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 87026.4219 - val_loss: 122997.1172\n",
      "Epoch 88/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 84010.8125 - val_loss: 115145.0859\n",
      "Epoch 89/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 83969.3125 - val_loss: 134356.4844\n",
      "Epoch 90/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82153.7656 - val_loss: 114368.3516\n",
      "Epoch 91/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82601.5781 - val_loss: 118764.4453\n",
      "Epoch 92/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82877.4609 - val_loss: 128123.3125\n",
      "Epoch 93/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82180.7734 - val_loss: 105184.8672\n",
      "Epoch 94/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 83470.3672 - val_loss: 126046.1875\n",
      "Epoch 95/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82983.0781 - val_loss: 124188.5703\n",
      "Epoch 96/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 81953.3438 - val_loss: 127226.7891\n",
      "Epoch 97/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 81873.7578 - val_loss: 124549.5703\n",
      "Epoch 98/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 79971.3672 - val_loss: 131709.2500\n",
      "Epoch 99/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82911.4219 - val_loss: 117523.8750\n",
      "Epoch 100/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 79472.3594 - val_loss: 123663.5000\n",
      "Epoch 101/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82320.6406 - val_loss: 116600.6406\n",
      "Epoch 102/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 80061.1250 - val_loss: 135201.5781\n",
      "Epoch 103/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 83790.3359 - val_loss: 129078.1719\n",
      "Epoch 104/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82902.5078 - val_loss: 109667.7734\n",
      "Epoch 105/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 79463.2969 - val_loss: 121308.5547\n",
      "Epoch 106/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 83622.1484 - val_loss: 106343.1172\n",
      "Epoch 107/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 82727.6562 - val_loss: 109329.7109\n",
      "Epoch 108/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 78308.7656 - val_loss: 111580.6250\n",
      "Epoch 109/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 76959.2734 - val_loss: 107319.2344\n",
      "Epoch 110/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 76581.2969 - val_loss: 107576.7578\n",
      "Epoch 111/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 76071.0625 - val_loss: 106298.4844\n",
      "Epoch 112/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75811.9453 - val_loss: 112142.7734\n",
      "Epoch 113/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75347.7344 - val_loss: 109858.3516\n",
      "Epoch 114/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75723.5312 - val_loss: 101829.8438\n",
      "Epoch 115/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75375.8750 - val_loss: 96620.6641\n",
      "Epoch 116/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 74911.4062 - val_loss: 108044.3984\n",
      "Epoch 117/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 74260.0938 - val_loss: 105434.0078\n",
      "Epoch 118/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 74660.3516 - val_loss: 113787.8906\n",
      "Epoch 119/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 76665.4453 - val_loss: 98598.2656\n",
      "Epoch 120/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 74882.2969 - val_loss: 123783.7969\n",
      "Epoch 121/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75432.2031 - val_loss: 115068.5312\n",
      "Epoch 122/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 73400.7188 - val_loss: 114975.8281\n",
      "Epoch 123/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 72245.9844 - val_loss: 116356.4219\n",
      "Epoch 124/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 72629.5625 - val_loss: 112399.8594\n",
      "Epoch 125/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 72049.2344 - val_loss: 97142.7578\n",
      "Epoch 126/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 73300.0078 - val_loss: 113937.2656\n",
      "Epoch 127/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 78687.4297 - val_loss: 98432.8516\n",
      "Epoch 128/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 75920.7188 - val_loss: 122121.9609\n",
      "Epoch 129/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 74469.1562 - val_loss: 113711.5391\n",
      "Epoch 130/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 71270.0938 - val_loss: 116701.8281\n",
      "Epoch 131/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 69931.6875 - val_loss: 116664.2344\n",
      "Epoch 132/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 70758.6562 - val_loss: 106262.7188\n",
      "Epoch 133/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 71937.3750 - val_loss: 94991.7109\n",
      "Epoch 134/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 77813.4688 - val_loss: 102165.3594\n",
      "Epoch 135/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 72294.5469 - val_loss: 118405.2734\n",
      "Epoch 136/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 69028.3750 - val_loss: 108431.2031\n",
      "Epoch 137/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68584.3359 - val_loss: 100947.8750\n",
      "Epoch 138/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68072.7891 - val_loss: 100331.1328\n",
      "Epoch 139/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68117.0859 - val_loss: 114176.0156\n",
      "Epoch 140/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 70142.6172 - val_loss: 88973.6172\n",
      "Epoch 141/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 70954.8906 - val_loss: 121246.7109\n",
      "Epoch 142/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 79245.4219 - val_loss: 115949.4297\n",
      "Epoch 143/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 70168.1406 - val_loss: 105174.0391\n",
      "Epoch 144/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 69861.1094 - val_loss: 104234.3281\n",
      "Epoch 145/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68360.2266 - val_loss: 95780.6641\n",
      "Epoch 146/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 67374.3672 - val_loss: 96044.6953\n",
      "Epoch 147/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 67699.8672 - val_loss: 92178.7188\n",
      "Epoch 148/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 66951.8750 - val_loss: 92950.1875\n",
      "Epoch 149/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 66789.0625 - val_loss: 93906.3750\n",
      "Epoch 150/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 67292.2656 - val_loss: 82568.1484\n",
      "Epoch 151/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 66486.4062 - val_loss: 89861.7578\n",
      "Epoch 152/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65700.6250 - val_loss: 107573.6016\n",
      "Epoch 153/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 69344.3125 - val_loss: 83189.5234\n",
      "Epoch 154/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68199.1250 - val_loss: 99980.4531\n",
      "Epoch 155/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 69787.2656 - val_loss: 83993.8438\n",
      "Epoch 156/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 66726.2344 - val_loss: 89124.3203\n",
      "Epoch 157/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68613.2109 - val_loss: 90384.5156\n",
      "Epoch 158/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65350.5547 - val_loss: 83100.5000\n",
      "Epoch 159/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65291.6055 - val_loss: 82220.9609\n",
      "Epoch 160/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65360.4375 - val_loss: 88286.4453\n",
      "Epoch 161/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 64895.1758 - val_loss: 100749.0469\n",
      "Epoch 162/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 67414.0859 - val_loss: 87805.4688\n",
      "Epoch 163/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 70816.9609 - val_loss: 93609.9453\n",
      "Epoch 164/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 71723.3047 - val_loss: 89153.0469\n",
      "Epoch 165/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68151.3516 - val_loss: 89451.4922\n",
      "Epoch 166/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65172.3359 - val_loss: 83037.7656\n",
      "Epoch 167/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 63251.0391 - val_loss: 81967.2344\n",
      "Epoch 168/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 63650.4961 - val_loss: 84226.4922\n",
      "Epoch 169/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62576.1523 - val_loss: 84779.3984\n",
      "Epoch 170/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62332.2773 - val_loss: 82329.9609\n",
      "Epoch 171/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62023.7031 - val_loss: 80608.0234\n",
      "Epoch 172/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62602.0781 - val_loss: 79116.5547\n",
      "Epoch 173/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62012.5664 - val_loss: 84574.3281\n",
      "Epoch 174/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 63402.7422 - val_loss: 91560.3672\n",
      "Epoch 175/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 64989.6094 - val_loss: 85608.7188\n",
      "Epoch 176/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62904.7500 - val_loss: 81064.1172\n",
      "Epoch 177/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62380.3945 - val_loss: 86168.0547\n",
      "Epoch 178/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61438.1250 - val_loss: 85032.1016\n",
      "Epoch 179/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61578.1445 - val_loss: 83761.4922\n",
      "Epoch 180/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62247.9375 - val_loss: 90921.4922\n",
      "Epoch 181/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62498.1133 - val_loss: 94914.6016\n",
      "Epoch 182/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 64361.7266 - val_loss: 97082.2500\n",
      "Epoch 183/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 68486.9531 - val_loss: 95496.9453\n",
      "Epoch 184/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 64019.5156 - val_loss: 84667.1250\n",
      "Epoch 185/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 65155.5742 - val_loss: 83810.4453\n",
      "Epoch 186/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61934.3164 - val_loss: 92648.3359\n",
      "Epoch 187/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60918.1250 - val_loss: 79246.7812\n",
      "Epoch 188/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61705.2891 - val_loss: 76450.5234\n",
      "Epoch 189/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61626.3203 - val_loss: 84138.2578\n",
      "Epoch 190/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62660.2734 - val_loss: 83472.9922\n",
      "Epoch 191/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60581.6641 - val_loss: 77290.1875\n",
      "Epoch 192/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60331.6953 - val_loss: 81820.0859\n",
      "Epoch 193/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 61263.8984 - val_loss: 79563.9766\n",
      "Epoch 194/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59746.7148 - val_loss: 78442.7734\n",
      "Epoch 195/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59564.1992 - val_loss: 76445.7812\n",
      "Epoch 196/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59088.7617 - val_loss: 77078.1719\n",
      "Epoch 197/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59085.4297 - val_loss: 78883.7656\n",
      "Epoch 198/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59662.6445 - val_loss: 80914.9688\n",
      "Epoch 199/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61087.7969 - val_loss: 82736.4297\n",
      "Epoch 200/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60707.9844 - val_loss: 81693.1484\n",
      "Epoch 201/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62340.0039 - val_loss: 79254.7812\n",
      "Epoch 202/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60172.7695 - val_loss: 98728.8594\n",
      "Epoch 203/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62167.2188 - val_loss: 101084.2344\n",
      "Epoch 204/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62328.8477 - val_loss: 82553.9453\n",
      "Epoch 205/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62594.6055 - val_loss: 79378.7266\n",
      "Epoch 206/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59936.4844 - val_loss: 91973.8516\n",
      "Epoch 207/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58637.3984 - val_loss: 92047.6328\n",
      "Epoch 208/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59405.4805 - val_loss: 76604.1016\n",
      "Epoch 209/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58713.4531 - val_loss: 78061.0859\n",
      "Epoch 210/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58674.6914 - val_loss: 75762.1719\n",
      "Epoch 211/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58431.4258 - val_loss: 79899.2422\n",
      "Epoch 212/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57631.9375 - val_loss: 83080.8125\n",
      "Epoch 213/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59189.5156 - val_loss: 79411.0703\n",
      "Epoch 214/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58831.3203 - val_loss: 79213.0781\n",
      "Epoch 215/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 57899.8320 - val_loss: 76704.2891\n",
      "Epoch 216/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57503.9062 - val_loss: 74757.3281\n",
      "Epoch 217/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57075.8242 - val_loss: 79133.8672\n",
      "Epoch 218/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56106.1211 - val_loss: 89213.3359\n",
      "Epoch 219/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57422.4883 - val_loss: 86285.7891\n",
      "Epoch 220/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59234.9570 - val_loss: 97200.6641\n",
      "Epoch 221/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60695.2031 - val_loss: 82313.3906\n",
      "Epoch 222/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59864.2656 - val_loss: 81158.8125\n",
      "Epoch 223/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58600.4023 - val_loss: 104135.7891\n",
      "Epoch 224/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59013.5781 - val_loss: 89481.9375\n",
      "Epoch 225/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58248.3086 - val_loss: 76714.3203\n",
      "Epoch 226/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58502.0078 - val_loss: 87011.7422\n",
      "Epoch 227/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59882.4180 - val_loss: 78090.0391\n",
      "Epoch 228/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56845.4727 - val_loss: 90178.2734\n",
      "Epoch 229/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56217.0508 - val_loss: 89711.2578\n",
      "Epoch 230/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57061.7227 - val_loss: 85455.9375\n",
      "Epoch 231/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55987.8281 - val_loss: 78353.9297\n",
      "Epoch 232/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55706.2461 - val_loss: 75773.0703\n",
      "Epoch 233/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57116.2539 - val_loss: 75635.2109\n",
      "Epoch 234/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58182.6211 - val_loss: 85819.5391\n",
      "Epoch 235/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61375.3789 - val_loss: 85439.0859\n",
      "Epoch 236/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56802.4883 - val_loss: 73237.6484\n",
      "Epoch 237/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55152.3867 - val_loss: 72584.7734\n",
      "Epoch 238/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54091.9297 - val_loss: 72323.4531\n",
      "Epoch 239/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53821.6484 - val_loss: 75023.0703\n",
      "Epoch 240/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53708.5742 - val_loss: 74779.8125\n",
      "Epoch 241/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55073.6367 - val_loss: 76130.2422\n",
      "Epoch 242/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55835.0898 - val_loss: 87206.8906\n",
      "Epoch 243/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 62654.9727 - val_loss: 73877.4922\n",
      "Epoch 244/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55591.7422 - val_loss: 78873.8750\n",
      "Epoch 245/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55585.1133 - val_loss: 89061.3750\n",
      "Epoch 246/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57911.3008 - val_loss: 90852.0156\n",
      "Epoch 247/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56140.4688 - val_loss: 87687.5000\n",
      "Epoch 248/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54885.1641 - val_loss: 74683.5938\n",
      "Epoch 249/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55602.3906 - val_loss: 77608.6875\n",
      "Epoch 250/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54685.2812 - val_loss: 87427.6875\n",
      "Epoch 251/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53607.0664 - val_loss: 91812.1250\n",
      "Epoch 252/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53090.4570 - val_loss: 82664.4062\n",
      "Epoch 253/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53063.0938 - val_loss: 81052.8750\n",
      "Epoch 254/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52808.0156 - val_loss: 79708.6797\n",
      "Epoch 255/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52555.5703 - val_loss: 79388.9375\n",
      "Epoch 256/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53252.0391 - val_loss: 72792.8047\n",
      "Epoch 257/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55452.4180 - val_loss: 83543.1719\n",
      "Epoch 258/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56325.7695 - val_loss: 91574.8047\n",
      "Epoch 259/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 60582.3125 - val_loss: 75679.9766\n",
      "Epoch 260/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57329.8047 - val_loss: 80919.5391\n",
      "Epoch 261/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54460.7070 - val_loss: 85683.5078\n",
      "Epoch 262/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51958.6445 - val_loss: 85720.4141\n",
      "Epoch 263/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51498.4961 - val_loss: 85176.1719\n",
      "Epoch 264/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51404.9336 - val_loss: 80597.3672\n",
      "Epoch 265/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51438.5156 - val_loss: 74145.9453\n",
      "Epoch 266/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51240.7148 - val_loss: 75014.0000\n",
      "Epoch 267/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51455.2383 - val_loss: 79576.7031\n",
      "Epoch 268/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51968.9258 - val_loss: 76488.3906\n",
      "Epoch 269/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55646.8750 - val_loss: 80491.2422\n",
      "Epoch 270/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57581.4570 - val_loss: 77264.2344\n",
      "Epoch 271/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58190.1953 - val_loss: 73548.1719\n",
      "Epoch 272/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53996.1328 - val_loss: 77694.1250\n",
      "Epoch 273/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52518.2070 - val_loss: 81161.0625\n",
      "Epoch 274/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51307.6602 - val_loss: 76723.9688\n",
      "Epoch 275/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50931.7773 - val_loss: 72947.8438\n",
      "Epoch 276/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50885.4570 - val_loss: 70951.1406\n",
      "Epoch 277/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50891.6367 - val_loss: 80128.5625\n",
      "Epoch 278/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52251.8008 - val_loss: 81546.1094\n",
      "Epoch 279/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52196.3125 - val_loss: 83459.1719\n",
      "Epoch 280/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52137.1992 - val_loss: 76726.7109\n",
      "Epoch 281/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53830.1758 - val_loss: 73334.8125\n",
      "Epoch 282/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51972.9023 - val_loss: 76058.6719\n",
      "Epoch 283/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51687.8320 - val_loss: 74976.0391\n",
      "Epoch 284/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51153.8594 - val_loss: 83241.0859\n",
      "Epoch 285/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 53281.0586 - val_loss: 90443.4609\n",
      "Epoch 286/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54899.0938 - val_loss: 83281.3672\n",
      "Epoch 287/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 56357.6602 - val_loss: 80772.6250\n",
      "Epoch 288/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 55017.0273 - val_loss: 90882.1406\n",
      "Epoch 289/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 57226.8438 - val_loss: 89414.6641\n",
      "Epoch 290/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 59515.0352 - val_loss: 89412.8047\n",
      "Epoch 291/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52926.4531 - val_loss: 83858.7266\n",
      "Epoch 292/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51259.5938 - val_loss: 78135.0781\n",
      "Epoch 293/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51019.8164 - val_loss: 75779.4766\n",
      "Epoch 294/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50701.1562 - val_loss: 75811.5625\n",
      "Epoch 295/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50912.7344 - val_loss: 74489.5156\n",
      "Epoch 296/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50810.4727 - val_loss: 76055.5625\n",
      "Epoch 297/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50428.1055 - val_loss: 74354.0469\n",
      "Epoch 298/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49822.5273 - val_loss: 74592.2500\n",
      "Epoch 299/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50102.2422 - val_loss: 70927.2578\n",
      "Epoch 300/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52043.1367 - val_loss: 78988.4922\n",
      "Epoch 301/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 52563.6641 - val_loss: 80201.8281\n",
      "Epoch 302/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54057.6133 - val_loss: 79845.9375\n",
      "Epoch 303/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54788.9102 - val_loss: 98791.1484\n",
      "Epoch 304/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50515.8984 - val_loss: 96262.3906\n",
      "Epoch 305/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49584.9375 - val_loss: 86636.1250\n",
      "Epoch 306/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49077.7734 - val_loss: 84730.4688\n",
      "Epoch 307/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48896.4375 - val_loss: 78535.0078\n",
      "Epoch 308/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49056.4648 - val_loss: 83161.6641\n",
      "Epoch 309/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49540.3789 - val_loss: 73328.7812\n",
      "Epoch 310/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50151.3281 - val_loss: 70125.7891\n",
      "Epoch 311/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50155.2891 - val_loss: 78993.1484\n",
      "Epoch 312/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51093.0703 - val_loss: 71734.1172\n",
      "Epoch 313/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49884.1289 - val_loss: 121183.9844\n",
      "Epoch 314/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 58345.1953 - val_loss: 70180.2188\n",
      "Epoch 315/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49427.8672 - val_loss: 70626.5781\n",
      "Epoch 316/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48533.5586 - val_loss: 68488.0000\n",
      "Epoch 317/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48537.6016 - val_loss: 69827.4766\n",
      "Epoch 318/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47596.4727 - val_loss: 75428.5156\n",
      "Epoch 319/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47948.9141 - val_loss: 79245.1719\n",
      "Epoch 320/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47366.0312 - val_loss: 73039.7188\n",
      "Epoch 321/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47541.0586 - val_loss: 76700.7578\n",
      "Epoch 322/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47622.7852 - val_loss: 75733.5469\n",
      "Epoch 323/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48822.3633 - val_loss: 77449.0391\n",
      "Epoch 324/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53514.6445 - val_loss: 74502.0000\n",
      "Epoch 325/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53110.6172 - val_loss: 86975.9609\n",
      "Epoch 326/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49414.4336 - val_loss: 98619.1250\n",
      "Epoch 327/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49310.9062 - val_loss: 75744.2266\n",
      "Epoch 328/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48100.6133 - val_loss: 71482.8203\n",
      "Epoch 329/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48080.3594 - val_loss: 75194.0391\n",
      "Epoch 330/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48199.5820 - val_loss: 70463.0469\n",
      "Epoch 331/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49126.8594 - val_loss: 74986.5859\n",
      "Epoch 332/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51174.0820 - val_loss: 70995.8438\n",
      "Epoch 333/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49857.0352 - val_loss: 73518.8594\n",
      "Epoch 334/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50044.8516 - val_loss: 67480.6250\n",
      "Epoch 335/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48324.4688 - val_loss: 85319.8438\n",
      "Epoch 336/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47704.3555 - val_loss: 81895.6484\n",
      "Epoch 337/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48988.4180 - val_loss: 73907.8750\n",
      "Epoch 338/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47513.9844 - val_loss: 68564.5391\n",
      "Epoch 339/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47429.0586 - val_loss: 78572.7422\n",
      "Epoch 340/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47367.5234 - val_loss: 76984.3203\n",
      "Epoch 341/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46685.5547 - val_loss: 68232.4844\n",
      "Epoch 342/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46989.7383 - val_loss: 66563.3359\n",
      "Epoch 343/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45644.6016 - val_loss: 72102.6719\n",
      "Epoch 344/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47453.1133 - val_loss: 74380.8281\n",
      "Epoch 345/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48592.5898 - val_loss: 91251.2734\n",
      "Epoch 346/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48485.1836 - val_loss: 89685.7969\n",
      "Epoch 347/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 45862.2148 - val_loss: 71870.0391\n",
      "Epoch 348/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46097.3906 - val_loss: 78190.5781\n",
      "Epoch 349/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45512.0078 - val_loss: 79572.2812\n",
      "Epoch 350/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45318.3086 - val_loss: 93058.6797\n",
      "Epoch 351/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45467.7305 - val_loss: 78395.8828\n",
      "Epoch 352/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46516.9531 - val_loss: 64664.3438\n",
      "Epoch 353/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48874.3711 - val_loss: 91585.7031\n",
      "Epoch 354/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 53652.5195 - val_loss: 74539.8828\n",
      "Epoch 355/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47483.0352 - val_loss: 73307.3906\n",
      "Epoch 356/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47096.3594 - val_loss: 71317.7500\n",
      "Epoch 357/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45186.0391 - val_loss: 75657.9375\n",
      "Epoch 358/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44488.9336 - val_loss: 76594.3281\n",
      "Epoch 359/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44175.5273 - val_loss: 74233.2812\n",
      "Epoch 360/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43850.6445 - val_loss: 72006.3750\n",
      "Epoch 361/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43962.0039 - val_loss: 70180.7109\n",
      "Epoch 362/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44617.6641 - val_loss: 70753.3672\n",
      "Epoch 363/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48293.8516 - val_loss: 76818.1719\n",
      "Epoch 364/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 49922.5703 - val_loss: 71885.1172\n",
      "Epoch 365/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46144.7188 - val_loss: 79684.8359\n",
      "Epoch 366/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45166.2031 - val_loss: 77578.4141\n",
      "Epoch 367/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45133.2773 - val_loss: 68759.5078\n",
      "Epoch 368/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44360.9766 - val_loss: 65918.3750\n",
      "Epoch 369/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43752.1992 - val_loss: 64299.7656\n",
      "Epoch 370/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45102.4648 - val_loss: 66431.2109\n",
      "Epoch 371/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47375.7500 - val_loss: 72409.1016\n",
      "Epoch 372/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54666.5547 - val_loss: 82157.1016\n",
      "Epoch 373/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 50615.2695 - val_loss: 79267.1875\n",
      "Epoch 374/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47250.5430 - val_loss: 94683.7812\n",
      "Epoch 375/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47522.1719 - val_loss: 96833.6172\n",
      "Epoch 376/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45142.4922 - val_loss: 93706.6250\n",
      "Epoch 377/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45106.0352 - val_loss: 79961.3906\n",
      "Epoch 378/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44848.2266 - val_loss: 85754.8906\n",
      "Epoch 379/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44816.7930 - val_loss: 76115.9844\n",
      "Epoch 380/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44188.4648 - val_loss: 73303.5703\n",
      "Epoch 381/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43173.3672 - val_loss: 71444.5703\n",
      "Epoch 382/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43360.2148 - val_loss: 74549.2891\n",
      "Epoch 383/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43884.5156 - val_loss: 71317.0156\n",
      "Epoch 384/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43543.8281 - val_loss: 72604.6406\n",
      "Epoch 385/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42958.0195 - val_loss: 69390.5703\n",
      "Epoch 386/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43842.8281 - val_loss: 71997.3750\n",
      "Epoch 387/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47595.9141 - val_loss: 77881.2969\n",
      "Epoch 388/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 61006.5820 - val_loss: 67971.2109\n",
      "Epoch 389/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 48761.2852 - val_loss: 70857.4688\n",
      "Epoch 390/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46066.4922 - val_loss: 72419.4766\n",
      "Epoch 391/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43879.1211 - val_loss: 78414.6250\n",
      "Epoch 392/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42946.2734 - val_loss: 72169.8906\n",
      "Epoch 393/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42373.8242 - val_loss: 71311.1094\n",
      "Epoch 394/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42113.0312 - val_loss: 69595.1641\n",
      "Epoch 395/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41885.8398 - val_loss: 68563.6172\n",
      "Epoch 396/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41663.9844 - val_loss: 66561.0078\n",
      "Epoch 397/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41577.0703 - val_loss: 66745.9688\n",
      "Epoch 398/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41571.5078 - val_loss: 69135.9844\n",
      "Epoch 399/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41643.9102 - val_loss: 62673.3008\n",
      "Epoch 400/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43233.3906 - val_loss: 84738.3750\n",
      "Epoch 401/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 54967.3906 - val_loss: 64940.7227\n",
      "Epoch 402/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46435.5977 - val_loss: 61058.6211\n",
      "Epoch 403/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44553.3672 - val_loss: 68361.6094\n",
      "Epoch 404/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44287.2227 - val_loss: 72147.2578\n",
      "Epoch 405/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43613.1367 - val_loss: 64835.6367\n",
      "Epoch 406/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42466.1016 - val_loss: 59514.5469\n",
      "Epoch 407/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41602.2109 - val_loss: 70980.2500\n",
      "Epoch 408/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42124.8047 - val_loss: 72731.8984\n",
      "Epoch 409/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42323.8555 - val_loss: 63791.3008\n",
      "Epoch 410/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42218.9062 - val_loss: 61545.5625\n",
      "Epoch 411/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43474.9766 - val_loss: 68434.1953\n",
      "Epoch 412/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45308.8711 - val_loss: 67001.7031\n",
      "Epoch 413/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46479.2266 - val_loss: 70650.9609\n",
      "Epoch 414/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43501.7188 - val_loss: 67606.3125\n",
      "Epoch 415/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43187.9258 - val_loss: 63702.7578\n",
      "Epoch 416/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 41941.3242 - val_loss: 70422.4141\n",
      "Epoch 417/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41758.3008 - val_loss: 65585.0234\n",
      "Epoch 418/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41555.0625 - val_loss: 58396.4492\n",
      "Epoch 419/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41565.0586 - val_loss: 64546.4297\n",
      "Epoch 420/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44337.9727 - val_loss: 68385.6719\n",
      "Epoch 421/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45894.5664 - val_loss: 83810.6953\n",
      "Epoch 422/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 49380.8086 - val_loss: 64039.6484\n",
      "Epoch 423/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44345.7930 - val_loss: 71267.1484\n",
      "Epoch 424/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42797.7266 - val_loss: 73100.9609\n",
      "Epoch 425/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41837.5234 - val_loss: 67889.4766\n",
      "Epoch 426/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41057.7266 - val_loss: 69656.7500\n",
      "Epoch 427/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41070.9336 - val_loss: 66667.4062\n",
      "Epoch 428/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41425.4336 - val_loss: 66377.1328\n",
      "Epoch 429/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40983.1016 - val_loss: 69339.0469\n",
      "Epoch 430/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 40876.1836 - val_loss: 66511.3047\n",
      "Epoch 431/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41190.2656 - val_loss: 63170.2422\n",
      "Epoch 432/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43255.2969 - val_loss: 63488.9922\n",
      "Epoch 433/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 45121.5898 - val_loss: 64314.1289\n",
      "Epoch 434/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42735.6914 - val_loss: 68007.0000\n",
      "Epoch 435/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42022.0898 - val_loss: 69487.8828\n",
      "Epoch 436/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41623.9492 - val_loss: 60920.0195\n",
      "Epoch 437/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41691.0586 - val_loss: 64175.5352\n",
      "Epoch 438/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41506.6328 - val_loss: 69950.4219\n",
      "Epoch 439/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40641.5352 - val_loss: 65148.0703\n",
      "Epoch 440/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40896.2578 - val_loss: 64349.3555\n",
      "Epoch 441/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42770.5312 - val_loss: 67898.7969\n",
      "Epoch 442/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43355.2812 - val_loss: 77681.6719\n",
      "Epoch 443/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 46830.9297 - val_loss: 69522.1484\n",
      "Epoch 444/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43859.7852 - val_loss: 97847.4688\n",
      "Epoch 445/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 51605.5508 - val_loss: 90392.3281\n",
      "Epoch 446/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43913.9648 - val_loss: 65470.1289\n",
      "Epoch 447/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41940.2852 - val_loss: 82376.6328\n",
      "Epoch 448/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41930.5039 - val_loss: 80605.7812\n",
      "Epoch 449/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41406.9180 - val_loss: 76482.1875\n",
      "Epoch 450/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41077.2109 - val_loss: 63152.9219\n",
      "Epoch 451/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40967.6523 - val_loss: 61687.3984\n",
      "Epoch 452/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40516.4141 - val_loss: 62937.6289\n",
      "Epoch 453/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40754.7031 - val_loss: 66736.0547\n",
      "Epoch 454/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41637.6328 - val_loss: 65416.3438\n",
      "Epoch 455/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43183.8750 - val_loss: 68784.5547\n",
      "Epoch 456/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 42935.5508 - val_loss: 67365.4219\n",
      "Epoch 457/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43622.6992 - val_loss: 69941.7188\n",
      "Epoch 458/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 44553.9648 - val_loss: 65757.0078\n",
      "Epoch 459/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41997.7422 - val_loss: 64789.5078\n",
      "Epoch 460/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41174.8906 - val_loss: 71165.1953\n",
      "Epoch 461/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 43924.5078 - val_loss: 74618.1953\n",
      "Epoch 462/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40501.7461 - val_loss: 65932.0625\n",
      "Epoch 463/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 39956.1719 - val_loss: 64614.9141\n",
      "Epoch 464/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 39390.9727 - val_loss: 60494.4570\n",
      "Epoch 465/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 39099.8203 - val_loss: 66567.9297\n",
      "Epoch 466/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 39787.1953 - val_loss: 74606.6641\n",
      "Epoch 467/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 39448.5703 - val_loss: 65427.2305\n",
      "Epoch 468/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 40683.3516 - val_loss: 67850.2188\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### # Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the model and get the training history with early stopping\n",
    "history = hybrid_sr_model.fit(\n",
    "    X_train_lr, \n",
    "    X_train_hr, \n",
    "    epochs=1000, \n",
    "    batch_size=4, \n",
    "    validation_data=(X_validation_lr, X_validation_hr),\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")\n",
    "\n",
    "# Visualize training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f6bef1-abb5-4b52-bf37-31525ce1da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Average PSNR on the test set: 31.409413190115064\n",
      "Average SSIM on the test set: 0.8751782\n",
      "Average SAM on the test set (in degrees): 4.175831499358239\n",
      "Average Correlation Coefficient on the test set: 0.9507240374727697\n",
      "Average ERGAS on the test set: 4.922399739071066\n",
      "Average RMSE: 0.027670536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def psnr(y_true, y_pred, max_pixel=None):\n",
    "    \"\"\"\n",
    "    Compute PSNR for each spectral band separately and return the average.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth image, shape (H, W, B)\n",
    "        y_pred: Super-resolved image, shape (H, W, B)\n",
    "        max_pixel: Maximum pixel value (None = use actual max from y_true)\n",
    "    \n",
    "    Returns:\n",
    "        Average PSNR across all bands\n",
    "    \"\"\"\n",
    "    if max_pixel is None:\n",
    "        max_pixel = np.max(y_true)  # Auto-detect max value if not provided\n",
    "\n",
    "    B = y_true.shape[-1]  # Number of spectral bands\n",
    "    psnr_values = []\n",
    "    \n",
    "    for i in range(B):  # Loop over bands\n",
    "        mse = np.mean((y_true[..., i] - y_pred[..., i]) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr_values.append(float('inf'))  # Perfect reconstruction\n",
    "        else:\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "            psnr_values.append(psnr)\n",
    "    \n",
    "    return np.mean(psnr_values)  # Average across bands\n",
    "\n",
    "# Function to calculate SSIM with channel_axis\n",
    "def ssim_value(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape}\")\n",
    "    \n",
    "    data_range = y_true.max() - y_true.min()  # Calculate data range from y_true\n",
    "    ssim_val = ssim(y_true, y_pred, data_range=data_range, channel_axis=-1)\n",
    "    return ssim_val\n",
    "\n",
    "# Function to calculate Correlation Coefficient\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    corr_matrix = np.corrcoef(y_true_flat, y_pred_flat)\n",
    "    corr_value = corr_matrix[0, 1]\n",
    "    return corr_value\n",
    "\n",
    "# Function to calculate Spectral Angle Mapper (SAM) in degrees\n",
    "def sam(y_true, y_pred):\n",
    "    y_true_reshaped = y_true.reshape(-1, y_true.shape[-1])\n",
    "    y_pred_reshaped = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    non_zero_mask = (np.linalg.norm(y_true_reshaped, axis=1) > 1e-10) & (np.linalg.norm(y_pred_reshaped, axis=1) > 1e-10)\n",
    "    dot_product = np.sum(y_true_reshaped[non_zero_mask] * y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    norm_true = np.linalg.norm(y_true_reshaped[non_zero_mask], axis=1)\n",
    "    norm_pred = np.linalg.norm(y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        angles = np.arccos(np.clip(dot_product / (norm_true * norm_pred), -1.0, 1.0))\n",
    "    \n",
    "    if angles.size > 0:\n",
    "        sam_value_degrees = np.mean(angles) * (180 / np.pi)\n",
    "    else:\n",
    "        sam_value_degrees = 0\n",
    "    \n",
    "    return sam_value_degrees\n",
    "\n",
    "# Function to normalize the images\n",
    "def normalize(image):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE) for hyperspectral images (normalized)\n",
    "def rmse_bandwise(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch between true and predicted images.\")\n",
    "    \n",
    "    bands = y_true.shape[-1]\n",
    "    rmse_per_band = []\n",
    "\n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        \n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)\n",
    "        rmse_band_value = np.sqrt(mse_band)\n",
    "        rmse_per_band.append(rmse_band_value)\n",
    "\n",
    "    # Normalize RMSE by the maximum value in y_true across all bands\n",
    "    max_value = np.max(y_true)\n",
    "    normalized_rmse = np.mean(rmse_per_band) / max_value\n",
    "    return normalized_rmse\n",
    "\n",
    "# Function to calculate ERGAS\n",
    "def ergas(y_true, y_pred, scale):\n",
    "    bands = y_true.shape[-1]\n",
    "    ergas_value = 0\n",
    "    \n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        mean_band_true = np.mean(band_true)\n",
    "        \n",
    "        # Calculate RMSE for the band without using a separate function\n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)  # Mean Squared Error for the band\n",
    "        rmse_band = np.sqrt(mse_band)  # Root Mean Squared Error for the band\n",
    "        \n",
    "        ergas_value += (rmse_band / mean_band_true) ** 2\n",
    "    \n",
    "    ergas_value = 100 * (1 / scale) * np.sqrt(ergas_value / bands)\n",
    "    return ergas_value\n",
    "\n",
    "# Assuming hybrid_sr_model is trained, and X_test_lr, X_test_hr are defined\n",
    "predicted_hr_images = hybrid_sr_model.predict(X_test_lr, batch_size=4)\n",
    "\n",
    "downscale_factor = 4 # ERGAS downscale factor\n",
    "\n",
    "# Validate shapes match for test and predictions\n",
    "if predicted_hr_images.shape != X_test_hr.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_hr_images shape {predicted_hr_images.shape} vs X_test_hr shape {X_test_hr.shape}\")\n",
    "\n",
    "# Calculate metrics per test sample\n",
    "psnr_values, ssim_values, cc_values, sam_values, ergas_values, rmse_values = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test_hr)):\n",
    "    psnr_values.append(psnr(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ssim_values.append(ssim_value(X_test_hr[i], predicted_hr_images[i]))\n",
    "    cc_values.append(correlation_coefficient(X_test_hr[i], predicted_hr_images[i]))\n",
    "    sam_values.append(sam(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ergas_values.append(ergas(X_test_hr[i], predicted_hr_images[i], downscale_factor))\n",
    "    rmse_values.append(rmse_bandwise(X_test_hr[i], predicted_hr_images[i]))\n",
    "\n",
    "# Average metrics\n",
    "average_psnr = np.mean(psnr_values)\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_cc = np.mean(cc_values)\n",
    "average_sam = np.mean(sam_values)\n",
    "average_ergas = np.mean(ergas_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Average PSNR on the test set:\", average_psnr)\n",
    "print(\"Average SSIM on the test set:\", average_ssim)\n",
    "print(\"Average SAM on the test set (in degrees):\", average_sam)\n",
    "print(\"Average Correlation Coefficient on the test set:\", average_cc)\n",
    "print(\"Average ERGAS on the test set:\", average_ergas)\n",
    "print(\"Average RMSE:\", average_rmse)  # Indicate RMSE is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfb77-74c1-4c34-825b-e309932acfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

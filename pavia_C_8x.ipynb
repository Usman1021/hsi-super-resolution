{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8d36ef-2dff-4215-b798-08f878057a3d",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Model for HyperspectralvImage Super-resolution with Gradient-Aware Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e0d5a-f4e5-472a-a2f2-e0c16e43f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from spectral import open_image\n",
    "from scipy.io import loadmat \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, Activation, Add, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646b955-67fe-4c08-8287-fc146607c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all seeds for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Ensure TensorFlow uses deterministic operations\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load the Pavia dataset\n",
    "try:\n",
    "    data = loadmat(\"PaviaU.mat\")  # Ensure that the file path is correct\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading .mat file: {e}\")\n",
    "\n",
    "# Access the hyperspectral image using the correct key 'paviaU'\n",
    "print(\"Keys in loaded .mat file:\", data.keys())\n",
    "if 'paviaU' in data:\n",
    "    hyperspectral_image = data['paviaU']\n",
    "else:\n",
    "    raise KeyError(\"'paviaU' not found in the .mat file.\")\n",
    "\n",
    "# Check the shape of the hyperspectral image\n",
    "print(\"Hyperspectral image shape:\", hyperspectral_image.shape)\n",
    "\n",
    "# Convert to float32 for TensorFlow operations\n",
    "hyperspectral_image = hyperspectral_image.astype(np.float32)\n",
    "\n",
    "# Parameters\n",
    "patch_size = (144, 144)  # Size of patches to extract\n",
    "test_size = 0.2  # Proportion of data for testing\n",
    "validation_size = 0.1  # Proportion of data for validation\n",
    "downscale_factor = 8  # Factor to downscale patches\n",
    "nodata_value = -1  # Value that indicates \"no data\"\n",
    "group_size = 6  # Group size for spectral bands\n",
    "overlap_size = 2  # Overlap size for grouped bands\n",
    "\n",
    "# Function to group bands into overlapping subgroups\n",
    "def group_bands_with_overlap(data, group_size=6, overlap_size=2):\n",
    "    height, width, bands = data.shape\n",
    "    step_size = group_size - overlap_size  # Calculate step size based on overlap\n",
    "    grouped_data = []\n",
    "\n",
    "    # Create overlapping groups of bands\n",
    "    for g in range(0, bands - group_size + 1, step_size):\n",
    "        group = data[:, :, g:g + group_size]\n",
    "        grouped_data.append(group)\n",
    "    \n",
    "    return np.array(grouped_data)\n",
    "\n",
    "# Extract and downscale patches from hyperspectral data\n",
    "def extract_and_downscale_patches(data, patch_size, downscale_factor, nodata_value=0):\n",
    "    patches_hr = []\n",
    "    patches_lr = []\n",
    "    height, width, bands = data.shape\n",
    "\n",
    "    for i in range(0, height - patch_size[0] + 1, patch_size[0]):\n",
    "        for j in range(0, width - patch_size[1] + 1, patch_size[1]):\n",
    "            patch_hr = data[i:i + patch_size[0], j:j + patch_size[1], :]\n",
    "\n",
    "            # Check for nodata_value and skip patch extraction if present\n",
    "            if np.any(patch_hr == nodata_value):\n",
    "                continue\n",
    "            \n",
    "            patch_lr = tf.image.resize(patch_hr, \n",
    "                                       [patch_size[0] // downscale_factor, patch_size[1] // downscale_factor], \n",
    "                                       method='bilinear')\n",
    "            patches_hr.append(patch_hr)\n",
    "            patches_lr.append(patch_lr.numpy())  # Convert tensor to numpy\n",
    "\n",
    "    return np.array(patches_hr), np.array(patches_lr)\n",
    "\n",
    "# Group bands into overlapping subgroups\n",
    "grouped_data = group_bands_with_overlap(hyperspectral_image, group_size=group_size, overlap_size=overlap_size)\n",
    "\n",
    "# Extract and downscale patches for all groups\n",
    "all_patches_hr = []\n",
    "all_patches_lr = []\n",
    "\n",
    "for group in grouped_data:\n",
    "    patches_hr, patches_lr = extract_and_downscale_patches(group, patch_size, downscale_factor, nodata_value=nodata_value)\n",
    "    all_patches_hr.append(patches_hr)\n",
    "    all_patches_lr.append(patches_lr)\n",
    "\n",
    "# Convert lists to numpy arrays before shuffling\n",
    "all_patches_hr = np.array(all_patches_hr)\n",
    "all_patches_lr = np.array(all_patches_lr)\n",
    "\n",
    "# Concatenate patches from all groups\n",
    "all_patches_hr = np.concatenate(all_patches_hr, axis=0)\n",
    "all_patches_lr = np.concatenate(all_patches_lr, axis=0)\n",
    "\n",
    "# Calculate the number of patches\n",
    "num_patches = len(all_patches_hr)\n",
    "\n",
    "# Calculate sizes for training, validation, and testing sets\n",
    "train_size = int((1 - test_size - validation_size) * num_patches)\n",
    "validation_size = int(validation_size * num_patches)\n",
    "test_size = num_patches - (train_size + validation_size)  # Explicit calculation of test size\n",
    "\n",
    "# Shuffle indices for splitting the data\n",
    "indices = np.arange(num_patches)\n",
    "np.random.shuffle(indices)\n",
    "all_patches_hr = all_patches_hr[indices]\n",
    "all_patches_lr = all_patches_lr[indices]\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "X_train_hr, X_validation_hr, X_test_hr = np.split(all_patches_hr, [train_size, train_size + validation_size])\n",
    "X_train_lr, X_validation_lr, X_test_lr = np.split(all_patches_lr, [train_size, train_size + validation_size])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X_train_hr shape:\", X_train_hr.shape)\n",
    "print(\"X_validation_hr shape:\", X_validation_hr.shape)\n",
    "print(\"X_test_hr shape:\", X_test_hr.shape)\n",
    "\n",
    "print(\"X_train_lr shape:\", X_train_lr.shape)\n",
    "print(\"X_validation_lr shape:\", X_validation_lr.shape)\n",
    "print(\"X_test_lr shape:\", X_test_lr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38a918-f8fa-4ee6-9d67-0512babe7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spatial–Spectral Gradient Loss Function\n",
    "def spatial_spectral_gradient_loss(y_true, y_pred):\n",
    "    mse_loss = K.mean(K.square(y_true - y_pred))\n",
    "    \n",
    "    def spatial_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "        grad_y_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
    "        grad_y_true_y = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "        grad_y_pred_y = y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]\n",
    "        \n",
    "        loss_x = K.mean(K.square(grad_y_true_x - grad_y_pred_x))\n",
    "        loss_y = K.mean(K.square(grad_y_true_y - grad_y_pred_y))\n",
    "        \n",
    "        return loss_x + loss_y\n",
    "\n",
    "    def spectral_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_spectral = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
    "        grad_y_pred_spectral = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
    "        return K.mean(K.square(grad_y_true_spectral - grad_y_pred_spectral))\n",
    "\n",
    "    spatial_loss = spatial_gradient_loss(y_true, y_pred)\n",
    "    spectral_loss = spectral_gradient_loss(y_true, y_pred)\n",
    "    \n",
    "    total_loss = mse_loss + 0.1 * spatial_loss + 0.1 * spectral_loss\n",
    "    return total_loss\n",
    "\n",
    "# Residual Block\n",
    "def residual_block(x, filters=32):\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
    "    res = BatchNormalization()(res)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Spectral–Spatial Block\n",
    "def spectral_spatial_block(x, filters=32):\n",
    "    spatial = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    spatial = BatchNormalization()(spatial)\n",
    "    spatial = Activation('relu')(spatial)\n",
    "    \n",
    "    spectral = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    spectral = BatchNormalization()(spectral)\n",
    "    spectral = Activation('relu')(spectral)\n",
    "    \n",
    "    spatial = Conv2D(filters, (1, 1), padding='same')(spatial)\n",
    "    \n",
    "    combined = Concatenate(axis=-1)([spatial, spectral])\n",
    "    return combined\n",
    "\n",
    "# Spectral Unmixing Block\n",
    "def spectral_unmixing_block(x, num_endmembers=10):\n",
    "    x = Conv2D(num_endmembers, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return x\n",
    "\n",
    "# General Upsampling Block (Supports Transpose and Standard Upsampling)\n",
    "def upsample_block(x, filters, scale=2, use_transpose=True):\n",
    "    if use_transpose:\n",
    "        x = Conv2DTranspose(filters, (3, 3), strides=(scale, scale), padding='same')(x)\n",
    "    else:\n",
    "        x = UpSampling2D(size=(scale, scale))(x)\n",
    "        x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build Model with Configurable Upsampling\n",
    "def build_hybrid_sr_model(input_shape, num_endmembers=40, use_transpose=True):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (9, 9), padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(16):\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    x = spectral_spatial_block(x)\n",
    "    \n",
    "    x_unmixed = spectral_unmixing_block(x, num_endmembers)\n",
    "    \n",
    "    x_concat = Concatenate(axis=-1)([x, x_unmixed])\n",
    "    \n",
    "    x_up = upsample_block(x_concat, filters=64, scale=2, use_transpose=use_transpose)  # 2x upscaling\n",
    "    x_up = upsample_block(x_up, filters=32, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    "    x_up = upsample_block(x_up, filters=16, scale=2, use_transpose=use_transpose)  # 8x upscaling\n",
    "    \n",
    "    x_out = Conv2D(input_shape[-1], (3, 3), padding='same')(x_up)\n",
    "    x_out = Activation('linear')(x_out)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x_out)\n",
    "    return model\n",
    "\n",
    "# Define input shape and build model\n",
    "input_shape = (18, 18, 6)\n",
    "num_endmembers = 40\n",
    "use_transpose = True  # Set to False if you want to use UpSampling2D instead\n",
    "\n",
    "hybrid_sr_model = build_hybrid_sr_model(input_shape, num_endmembers=num_endmembers, use_transpose=use_transpose)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "hybrid_sr_model.compile(optimizer='adam', loss=spatial_spectral_gradient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecace29e-d0a5-47f8-9a82-5419221f0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the model and get the training history with early stopping\n",
    "history = hybrid_sr_model.fit(\n",
    "    X_train_lr, \n",
    "    X_train_hr, \n",
    "    epochs=1000, \n",
    "    batch_size=4, \n",
    "    validation_data=(X_validation_lr, X_validation_hr),\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")\n",
    "\n",
    "# Visualize training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6bef1-abb5-4b52-bf37-31525ce1da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def psnr(y_true, y_pred, max_pixel=None):\n",
    "    \"\"\"\n",
    "    Compute PSNR for each spectral band separately and return the average.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth image, shape (H, W, B)\n",
    "        y_pred: Super-resolved image, shape (H, W, B)\n",
    "        max_pixel: Maximum pixel value (None = use actual max from y_true)\n",
    "    \n",
    "    Returns:\n",
    "        Average PSNR across all bands\n",
    "    \"\"\"\n",
    "    if max_pixel is None:\n",
    "        max_pixel = np.max(y_true)  # Auto-detect max value if not provided\n",
    "\n",
    "    B = y_true.shape[-1]  # Number of spectral bands\n",
    "    psnr_values = []\n",
    "    \n",
    "    for i in range(B):  # Loop over bands\n",
    "        mse = np.mean((y_true[..., i] - y_pred[..., i]) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr_values.append(float('inf'))  # Perfect reconstruction\n",
    "        else:\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "            psnr_values.append(psnr)\n",
    "    \n",
    "    return np.mean(psnr_values)  # Average across bands\n",
    "\n",
    "# Function to calculate SSIM with channel_axis\n",
    "def ssim_value(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape}\")\n",
    "    \n",
    "    data_range = y_true.max() - y_true.min()  # Calculate data range from y_true\n",
    "    ssim_val = ssim(y_true, y_pred, data_range=data_range, channel_axis=-1)\n",
    "    return ssim_val\n",
    "\n",
    "# Function to calculate Correlation Coefficient\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    corr_matrix = np.corrcoef(y_true_flat, y_pred_flat)\n",
    "    corr_value = corr_matrix[0, 1]\n",
    "    return corr_value\n",
    "\n",
    "# Function to calculate Spectral Angle Mapper (SAM) in degrees\n",
    "def sam(y_true, y_pred):\n",
    "    y_true_reshaped = y_true.reshape(-1, y_true.shape[-1])\n",
    "    y_pred_reshaped = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    non_zero_mask = (np.linalg.norm(y_true_reshaped, axis=1) > 1e-10) & (np.linalg.norm(y_pred_reshaped, axis=1) > 1e-10)\n",
    "    dot_product = np.sum(y_true_reshaped[non_zero_mask] * y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    norm_true = np.linalg.norm(y_true_reshaped[non_zero_mask], axis=1)\n",
    "    norm_pred = np.linalg.norm(y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        angles = np.arccos(np.clip(dot_product / (norm_true * norm_pred), -1.0, 1.0))\n",
    "    \n",
    "    if angles.size > 0:\n",
    "        sam_value_degrees = np.mean(angles) * (180 / np.pi)\n",
    "    else:\n",
    "        sam_value_degrees = 0\n",
    "    \n",
    "    return sam_value_degrees\n",
    "\n",
    "# Function to normalize the images\n",
    "def normalize(image):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE) for hyperspectral images (normalized)\n",
    "def rmse_bandwise(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch between true and predicted images.\")\n",
    "    \n",
    "    bands = y_true.shape[-1]\n",
    "    rmse_per_band = []\n",
    "\n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        \n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)\n",
    "        rmse_band_value = np.sqrt(mse_band)\n",
    "        rmse_per_band.append(rmse_band_value)\n",
    "\n",
    "    # Normalize RMSE by the maximum value in y_true across all bands\n",
    "    max_value = np.max(y_true)\n",
    "    normalized_rmse = np.mean(rmse_per_band) / max_value\n",
    "    return normalized_rmse\n",
    "\n",
    "# Function to calculate ERGAS\n",
    "def ergas(y_true, y_pred, scale):\n",
    "    bands = y_true.shape[-1]\n",
    "    ergas_value = 0\n",
    "    \n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        mean_band_true = np.mean(band_true)\n",
    "        \n",
    "        # Calculate RMSE for the band without using a separate function\n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)  # Mean Squared Error for the band\n",
    "        rmse_band = np.sqrt(mse_band)  # Root Mean Squared Error for the band\n",
    "        \n",
    "        ergas_value += (rmse_band / mean_band_true) ** 2\n",
    "    \n",
    "    ergas_value = 100 * (1 / scale) * np.sqrt(ergas_value / bands)\n",
    "    return ergas_value\n",
    "\n",
    "# Assuming hybrid_sr_model is trained, and X_test_lr, X_test_hr are defined\n",
    "predicted_hr_images = hybrid_sr_model.predict(X_test_lr, batch_size=4)\n",
    "\n",
    "downscale_factor = 8 # ERGAS downscale factor\n",
    "\n",
    "# Validate shapes match for test and predictions\n",
    "if predicted_hr_images.shape != X_test_hr.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_hr_images shape {predicted_hr_images.shape} vs X_test_hr shape {X_test_hr.shape}\")\n",
    "\n",
    "# Calculate metrics per test sample\n",
    "psnr_values, ssim_values, cc_values, sam_values, ergas_values, rmse_values = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test_hr)):\n",
    "    psnr_values.append(psnr(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ssim_values.append(ssim_value(X_test_hr[i], predicted_hr_images[i]))\n",
    "    cc_values.append(correlation_coefficient(X_test_hr[i], predicted_hr_images[i]))\n",
    "    sam_values.append(sam(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ergas_values.append(ergas(X_test_hr[i], predicted_hr_images[i], downscale_factor))\n",
    "    rmse_values.append(rmse_bandwise(X_test_hr[i], predicted_hr_images[i]))\n",
    "\n",
    "# Average metrics\n",
    "average_psnr = np.mean(psnr_values)\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_cc = np.mean(cc_values)\n",
    "average_sam = np.mean(sam_values)\n",
    "average_ergas = np.mean(ergas_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Average PSNR on the test set:\", average_psnr)\n",
    "print(\"Average SSIM on the test set:\", average_ssim)\n",
    "print(\"Average SAM on the test set (in degrees):\", average_sam)\n",
    "print(\"Average Correlation Coefficient on the test set:\", average_cc)\n",
    "print(\"Average ERGAS on the test set:\", average_ergas)\n",
    "print(\"Average RMSE:\", average_rmse)  # Indicate RMSE is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfb77-74c1-4c34-825b-e309932acfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

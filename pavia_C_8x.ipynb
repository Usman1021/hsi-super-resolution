{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8d36ef-2dff-4215-b798-08f878057a3d",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Model for HyperspectralvImage Super-resolution with Gradient-Aware Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67e0d5a-f4e5-472a-a2f2-e0c16e43f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:16:40.882330: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 16:16:40.897524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739888200.913808 1149840 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739888200.918904 1149840 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 16:16:40.936208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from spectral import open_image\n",
    "from scipy.io import loadmat \n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, Activation, Add, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f646b955-67fe-4c08-8287-fc146607c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in loaded .mat file: dict_keys(['__header__', '__version__', '__globals__', 'pavia'])\n",
      "Hyperspectral image shape: (1096, 715, 102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739888205.150683 1149840 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_hr shape: (490, 144, 144, 6)\n",
      "X_validation_hr shape: (70, 144, 144, 6)\n",
      "X_test_hr shape: (140, 144, 144, 6)\n",
      "X_train_lr shape: (490, 18, 18, 6)\n",
      "X_validation_lr shape: (70, 18, 18, 6)\n",
      "X_test_lr shape: (140, 18, 18, 6)\n"
     ]
    }
   ],
   "source": [
    "# Set all seeds for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Ensure TensorFlow uses deterministic operations\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Load the Pavia dataset\n",
    "try:\n",
    "    data = loadmat(\"Pavia.mat\")  # Ensure that the file path is correct\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading .mat file: {e}\")\n",
    "\n",
    "# Access the hyperspectral image using the correct key 'pavia'\n",
    "print(\"Keys in loaded .mat file:\", data.keys())\n",
    "if 'pavia' in data:\n",
    "    hyperspectral_image = data['pavia']\n",
    "else:\n",
    "    raise KeyError(\"'pavia' not found in the .mat file.\")\n",
    "\n",
    "# Check the shape of the hyperspectral image\n",
    "print(\"Hyperspectral image shape:\", hyperspectral_image.shape)\n",
    "\n",
    "# Convert to float32 for TensorFlow operations\n",
    "hyperspectral_image = hyperspectral_image.astype(np.float32)\n",
    "\n",
    "# Load the hyperspectral data using the spectral library\n",
    "data = hyperspectral_image  # Use the loaded hyperspectral image directly\n",
    "\n",
    "# Parameters\n",
    "patch_size = (144, 144)  # Size of patches to extract\n",
    "test_size = 0.2  # Proportion of data for testing\n",
    "validation_size = 0.1  # Proportion of data for validation\n",
    "downscale_factor = 8  # Factor to downscale patches\n",
    "nodata_value = -1  # Value that indicates \"no data\"\n",
    "group_size = 6  # Group size for spectral bands\n",
    "overlap_size = 2  # Overlap size for grouped bands\n",
    "\n",
    "# Function to group bands into overlapping subgroups\n",
    "def group_bands_with_overlap(data, group_size=6, overlap_size=2):\n",
    "    height, width, bands = data.shape\n",
    "    step_size = group_size - overlap_size  # Calculate step size based on overlap\n",
    "    grouped_data = []\n",
    "\n",
    "    # Create overlapping groups of bands\n",
    "    for g in range(0, bands - group_size + 1, step_size):\n",
    "        group = data[:, :, g:g + group_size]\n",
    "        grouped_data.append(group)\n",
    "    \n",
    "    return np.array(grouped_data)\n",
    "\n",
    "# Extract and downscale patches from hyperspectral data\n",
    "def extract_and_downscale_patches(data, patch_size, downscale_factor, nodata_value=0):\n",
    "    patches_hr = []\n",
    "    patches_lr = []\n",
    "    height, width, bands = data.shape\n",
    "\n",
    "    for i in range(0, height - patch_size[0] + 1, patch_size[0]):\n",
    "        for j in range(0, width - patch_size[1] + 1, patch_size[1]):\n",
    "            patch_hr = data[i:i + patch_size[0], j:j + patch_size[1], :]\n",
    "\n",
    "            # Check for nodata_value and skip patch extraction if present\n",
    "            if np.any(patch_hr == nodata_value):\n",
    "                continue\n",
    "            \n",
    "            patch_lr = tf.image.resize(patch_hr, \n",
    "                                       [patch_size[0] // downscale_factor, patch_size[1] // downscale_factor], \n",
    "                                       method='bilinear')\n",
    "            patches_hr.append(patch_hr)\n",
    "            patches_lr.append(patch_lr.numpy())  # Convert tensor to numpy\n",
    "\n",
    "    return np.array(patches_hr), np.array(patches_lr)\n",
    "\n",
    "# Group bands into overlapping subgroups\n",
    "grouped_data = group_bands_with_overlap(hyperspectral_image, group_size=group_size, overlap_size=overlap_size)\n",
    "\n",
    "# Extract and downscale patches for all groups\n",
    "all_patches_hr = []\n",
    "all_patches_lr = []\n",
    "\n",
    "for group in grouped_data:\n",
    "    patches_hr, patches_lr = extract_and_downscale_patches(group, patch_size, downscale_factor, nodata_value=nodata_value)\n",
    "    all_patches_hr.append(patches_hr)\n",
    "    all_patches_lr.append(patches_lr)\n",
    "\n",
    "# Convert lists to numpy arrays before shuffling\n",
    "all_patches_hr = np.array(all_patches_hr)\n",
    "all_patches_lr = np.array(all_patches_lr)\n",
    "\n",
    "# Concatenate patches from all groups\n",
    "all_patches_hr = np.concatenate(all_patches_hr, axis=0)\n",
    "all_patches_lr = np.concatenate(all_patches_lr, axis=0)\n",
    "\n",
    "# Calculate the number of patches\n",
    "num_patches = len(all_patches_hr)\n",
    "\n",
    "# Calculate sizes for training, validation, and testing sets\n",
    "train_size = int((1 - test_size - validation_size) * num_patches)\n",
    "validation_size = int(validation_size * num_patches)\n",
    "test_size = num_patches - (train_size + validation_size)  # Explicit calculation of test size\n",
    "\n",
    "# Shuffle indices for splitting the data\n",
    "indices = np.arange(num_patches)\n",
    "np.random.shuffle(indices)\n",
    "all_patches_hr = all_patches_hr[indices]\n",
    "all_patches_lr = all_patches_lr[indices]\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "X_train_hr, X_validation_hr, X_test_hr = np.split(all_patches_hr, [train_size, train_size + validation_size])\n",
    "X_train_lr, X_validation_lr, X_test_lr = np.split(all_patches_lr, [train_size, train_size + validation_size])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X_train_hr shape:\", X_train_hr.shape)\n",
    "print(\"X_validation_hr shape:\", X_validation_hr.shape)\n",
    "print(\"X_test_hr shape:\", X_test_hr.shape)\n",
    "\n",
    "print(\"X_train_lr shape:\", X_train_lr.shape)\n",
    "print(\"X_validation_lr shape:\", X_validation_lr.shape)\n",
    "print(\"X_test_lr shape:\", X_test_lr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38a918-f8fa-4ee6-9d67-0512babe7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spatial–Spectral Gradient Loss Function\n",
    "def spatial_spectral_gradient_loss(y_true, y_pred):\n",
    "    mse_loss = K.mean(K.square(y_true - y_pred))\n",
    "    \n",
    "    def spatial_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "        grad_y_pred_x = y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :]\n",
    "        grad_y_true_y = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "        grad_y_pred_y = y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]\n",
    "        \n",
    "        loss_x = K.mean(K.square(grad_y_true_x - grad_y_pred_x))\n",
    "        loss_y = K.mean(K.square(grad_y_true_y - grad_y_pred_y))\n",
    "        \n",
    "        return loss_x + loss_y\n",
    "\n",
    "    def spectral_gradient_loss(y_true, y_pred):\n",
    "        grad_y_true_spectral = y_true[:, :, :, 1:] - y_true[:, :, :, :-1]\n",
    "        grad_y_pred_spectral = y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1]\n",
    "        return K.mean(K.square(grad_y_true_spectral - grad_y_pred_spectral))\n",
    "\n",
    "    spatial_loss = spatial_gradient_loss(y_true, y_pred)\n",
    "    spectral_loss = spectral_gradient_loss(y_true, y_pred)\n",
    "    \n",
    "    total_loss = mse_loss + 0.1 * spatial_loss + 0.1 * spectral_loss\n",
    "    return total_loss\n",
    "\n",
    "# Residual Block\n",
    "def residual_block(x, filters=32):\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
    "    res = BatchNormalization()(res)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Spectral–Spatial Block\n",
    "def spectral_spatial_block(x, filters=32):\n",
    "    spatial = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    spatial = BatchNormalization()(spatial)\n",
    "    spatial = Activation('relu')(spatial)\n",
    "    \n",
    "    spectral = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    spectral = BatchNormalization()(spectral)\n",
    "    spectral = Activation('relu')(spectral)\n",
    "    \n",
    "    spatial = Conv2D(filters, (1, 1), padding='same')(spatial)\n",
    "    \n",
    "    combined = Concatenate(axis=-1)([spatial, spectral])\n",
    "    return combined\n",
    "\n",
    "# Spectral Unmixing Block\n",
    "def spectral_unmixing_block(x, num_endmembers=10):\n",
    "    x = Conv2D(num_endmembers, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return x\n",
    "\n",
    "# General Upsampling Block (Supports Transpose and Standard Upsampling)\n",
    "def upsample_block(x, filters, scale=2, use_transpose=True):\n",
    "    if use_transpose:\n",
    "        x = Conv2DTranspose(filters, (3, 3), strides=(scale, scale), padding='same')(x)\n",
    "    else:\n",
    "        x = UpSampling2D(size=(scale, scale))(x)\n",
    "        x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build Model with Configurable Upsampling\n",
    "def build_hybrid_sr_model(input_shape, num_endmembers=40, use_transpose=True):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (9, 9), padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    for _ in range(16):\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    x = spectral_spatial_block(x)\n",
    "    \n",
    "    x_unmixed = spectral_unmixing_block(x, num_endmembers)\n",
    "    \n",
    "    x_concat = Concatenate(axis=-1)([x, x_unmixed])\n",
    "    \n",
    "    x_up = upsample_block(x_concat, filters=64, scale=2, use_transpose=use_transpose)  # 2x upscaling\n",
    "    x_up = upsample_block(x_up, filters=32, scale=2, use_transpose=use_transpose)  # 4x upscaling\n",
    "    x_up = upsample_block(x_up, filters=16, scale=2, use_transpose=use_transpose)  # 8x upscaling\n",
    "    \n",
    "    x_out = Conv2D(input_shape[-1], (3, 3), padding='same')(x_up)\n",
    "    x_out = Activation('linear')(x_out)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x_out)\n",
    "    return model\n",
    "\n",
    "# Define input shape and build model\n",
    "input_shape = (18, 18, 6)\n",
    "num_endmembers = 40\n",
    "use_transpose = True  # Set to False if you want to use UpSampling2D instead\n",
    "\n",
    "hybrid_sr_model = build_hybrid_sr_model(input_shape, num_endmembers=num_endmembers, use_transpose=use_transpose)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "hybrid_sr_model.compile(optimizer='adam', loss=spatial_spectral_gradient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecace29e-d0a5-47f8-9a82-5419221f0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:16:48.388662: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739888222.985629 1149926 service.cc:148] XLA service 0x7f5a08013c10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739888222.985653 1149926 service.cc:156]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2025-02-18 16:17:03.461216: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1739888225.403423 1149926 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 14/123\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2265182.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739888232.243403 1149926 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2060297.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:17:20.870601: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 93ms/step - loss: 2059934.8750 - val_loss: 2276586.7500\n",
      "Epoch 2/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2006156.7500 - val_loss: 2188691.2500\n",
      "Epoch 3/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1925570.5000 - val_loss: 2070803.0000\n",
      "Epoch 4/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1818124.8750 - val_loss: 1945375.3750\n",
      "Epoch 5/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1687585.8750 - val_loss: 1775718.1250\n",
      "Epoch 6/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1539495.8750 - val_loss: 1554267.2500\n",
      "Epoch 7/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1381196.8750 - val_loss: 1362820.1250\n",
      "Epoch 8/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1219504.0000 - val_loss: 1167902.2500\n",
      "Epoch 9/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1061254.5000 - val_loss: 1002252.6875\n",
      "Epoch 10/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 911968.8750 - val_loss: 833509.4375\n",
      "Epoch 11/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 776410.7500 - val_loss: 675493.3750\n",
      "Epoch 12/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 657222.2500 - val_loss: 574732.5000\n",
      "Epoch 13/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 555952.4375 - val_loss: 499383.7188\n",
      "Epoch 14/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 471776.1562 - val_loss: 404462.2500\n",
      "Epoch 15/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 405032.8438 - val_loss: 357075.8438\n",
      "Epoch 16/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 353452.4375 - val_loss: 333211.5312\n",
      "Epoch 17/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 315849.3438 - val_loss: 301435.1562\n",
      "Epoch 18/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 288295.5625 - val_loss: 275421.5938\n",
      "Epoch 19/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 267167.2500 - val_loss: 283674.1562\n",
      "Epoch 20/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 251959.4062 - val_loss: 259348.1719\n",
      "Epoch 21/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 239830.7656 - val_loss: 219658.3750\n",
      "Epoch 22/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 232092.8906 - val_loss: 210128.4688\n",
      "Epoch 23/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 222412.3438 - val_loss: 252023.3125\n",
      "Epoch 24/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 216298.2812 - val_loss: 261020.7969\n",
      "Epoch 25/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 209776.8750 - val_loss: 259653.0000\n",
      "Epoch 26/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 202224.5469 - val_loss: 232777.5312\n",
      "Epoch 27/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 196154.0625 - val_loss: 207091.7344\n",
      "Epoch 28/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 191402.6406 - val_loss: 193651.2344\n",
      "Epoch 29/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 188217.4688 - val_loss: 181389.6875\n",
      "Epoch 30/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 185666.8750 - val_loss: 175239.9219\n",
      "Epoch 31/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 183589.7500 - val_loss: 174387.7500\n",
      "Epoch 32/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 181553.6875 - val_loss: 173217.9375\n",
      "Epoch 33/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 179904.8750 - val_loss: 173812.9219\n",
      "Epoch 34/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 175942.8281 - val_loss: 175916.8594\n",
      "Epoch 35/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 173130.7812 - val_loss: 178818.7031\n",
      "Epoch 36/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 170406.5938 - val_loss: 179582.7969\n",
      "Epoch 37/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 168024.8594 - val_loss: 181990.3750\n",
      "Epoch 38/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 166006.8125 - val_loss: 188175.4219\n",
      "Epoch 39/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 164487.7656 - val_loss: 185237.1875\n",
      "Epoch 40/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 163200.4219 - val_loss: 185325.0000\n",
      "Epoch 41/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 162388.6250 - val_loss: 187284.5781\n",
      "Epoch 42/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 161299.4062 - val_loss: 189225.9688\n",
      "Epoch 43/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 160442.0000 - val_loss: 195159.1250\n",
      "Epoch 44/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 158231.7188 - val_loss: 198000.4531\n",
      "Epoch 45/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 157069.3750 - val_loss: 193986.9219\n",
      "Epoch 46/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 156445.6875 - val_loss: 184344.9219\n",
      "Epoch 47/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 158193.2344 - val_loss: 171852.9531\n",
      "Epoch 48/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 155658.4844 - val_loss: 174007.2812\n",
      "Epoch 49/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 152506.9688 - val_loss: 177316.3438\n",
      "Epoch 50/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 149423.3594 - val_loss: 176785.8906\n",
      "Epoch 51/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 147374.3438 - val_loss: 177199.8906\n",
      "Epoch 52/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 145349.6562 - val_loss: 175008.7500\n",
      "Epoch 53/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 143597.1406 - val_loss: 173416.3125\n",
      "Epoch 54/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 141637.1094 - val_loss: 167304.9844\n",
      "Epoch 55/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 140027.3438 - val_loss: 160919.9219\n",
      "Epoch 56/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 139403.7969 - val_loss: 172850.5469\n",
      "Epoch 57/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 139303.7969 - val_loss: 188204.5625\n",
      "Epoch 58/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 140020.6875 - val_loss: 182177.1875\n",
      "Epoch 59/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 139237.2188 - val_loss: 184575.4375\n",
      "Epoch 60/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 135634.8281 - val_loss: 184428.3594\n",
      "Epoch 61/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 136632.4219 - val_loss: 161416.0000\n",
      "Epoch 62/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 134110.8906 - val_loss: 159184.4219\n",
      "Epoch 63/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 132263.7031 - val_loss: 169692.2188\n",
      "Epoch 64/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 133141.0156 - val_loss: 169857.2969\n",
      "Epoch 65/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 132513.1094 - val_loss: 162906.1094\n",
      "Epoch 66/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 129873.8828 - val_loss: 198297.6406\n",
      "Epoch 67/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 131988.6875 - val_loss: 156857.9531\n",
      "Epoch 68/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 129213.9531 - val_loss: 184660.1094\n",
      "Epoch 69/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 132076.2188 - val_loss: 152130.8281\n",
      "Epoch 70/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 132682.0469 - val_loss: 250222.5469\n",
      "Epoch 71/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 132885.8281 - val_loss: 197175.8438\n",
      "Epoch 72/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 130848.2188 - val_loss: 161144.0312\n",
      "Epoch 73/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 128639.0938 - val_loss: 170148.4062\n",
      "Epoch 74/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 127269.6719 - val_loss: 243795.4062\n",
      "Epoch 75/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 126180.4219 - val_loss: 154455.8594\n",
      "Epoch 76/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 127546.9297 - val_loss: 205704.6562\n",
      "Epoch 77/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 124164.9844 - val_loss: 202232.2344\n",
      "Epoch 78/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 122980.2656 - val_loss: 172470.3906\n",
      "Epoch 79/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 121821.0312 - val_loss: 156564.6406\n",
      "Epoch 80/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 128615.0781 - val_loss: 192281.4531\n",
      "Epoch 81/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 125070.3516 - val_loss: 215801.6094\n",
      "Epoch 82/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 119466.5938 - val_loss: 185679.2812\n",
      "Epoch 83/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 120930.8516 - val_loss: 194129.1094\n",
      "Epoch 84/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 119557.9062 - val_loss: 147709.3281\n",
      "Epoch 85/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 119125.3906 - val_loss: 197579.1094\n",
      "Epoch 86/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 119069.7422 - val_loss: 200784.7969\n",
      "Epoch 87/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 115918.0391 - val_loss: 183201.2500\n",
      "Epoch 88/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 123263.0625 - val_loss: 150758.3750\n",
      "Epoch 89/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 124001.7109 - val_loss: 140099.2188\n",
      "Epoch 90/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 118184.7109 - val_loss: 192017.2500\n",
      "Epoch 91/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 116808.6797 - val_loss: 177724.5156\n",
      "Epoch 92/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 115280.8516 - val_loss: 161521.5156\n",
      "Epoch 93/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 114780.9922 - val_loss: 144616.8438\n",
      "Epoch 94/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 115276.0938 - val_loss: 158638.9375\n",
      "Epoch 95/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 111713.4531 - val_loss: 199764.9062\n",
      "Epoch 96/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 115805.2969 - val_loss: 178488.5781\n",
      "Epoch 97/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 112595.3984 - val_loss: 139586.0156\n",
      "Epoch 98/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 116937.1719 - val_loss: 143969.2500\n",
      "Epoch 99/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 114688.4219 - val_loss: 194019.9844\n",
      "Epoch 100/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 112025.1797 - val_loss: 159779.8594\n",
      "Epoch 101/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 108882.6641 - val_loss: 142626.6562\n",
      "Epoch 102/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 109794.3516 - val_loss: 144708.5938\n",
      "Epoch 103/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 107511.5078 - val_loss: 173966.7969\n",
      "Epoch 104/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 106464.8984 - val_loss: 182965.2500\n",
      "Epoch 105/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 106960.3125 - val_loss: 197811.9531\n",
      "Epoch 106/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 108627.4688 - val_loss: 141443.9062\n",
      "Epoch 107/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 113441.5078 - val_loss: 195920.5000\n",
      "Epoch 108/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 109293.4766 - val_loss: 198729.9844\n",
      "Epoch 109/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 108878.2578 - val_loss: 161687.0781\n",
      "Epoch 110/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 105422.2266 - val_loss: 158183.1250\n",
      "Epoch 111/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 104475.9531 - val_loss: 158547.0781\n",
      "Epoch 112/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 106116.5703 - val_loss: 169410.3281\n",
      "Epoch 113/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103480.9062 - val_loss: 148386.5781\n",
      "Epoch 114/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 106248.6328 - val_loss: 207019.3438\n",
      "Epoch 115/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 106815.4297 - val_loss: 195882.8438\n",
      "Epoch 116/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 108548.8203 - val_loss: 221087.5781\n",
      "Epoch 117/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 106533.5391 - val_loss: 178574.3906\n",
      "Epoch 118/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 104357.6250 - val_loss: 132284.5000\n",
      "Epoch 119/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 108487.7422 - val_loss: 164151.1094\n",
      "Epoch 120/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 108930.7344 - val_loss: 166114.3281\n",
      "Epoch 121/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103715.0547 - val_loss: 237300.5000\n",
      "Epoch 122/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 104709.5625 - val_loss: 191165.1406\n",
      "Epoch 123/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 101878.4688 - val_loss: 152132.9062\n",
      "Epoch 124/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103013.8203 - val_loss: 126893.0156\n",
      "Epoch 125/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 102596.9219 - val_loss: 221393.6250\n",
      "Epoch 126/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103805.0234 - val_loss: 208499.0469\n",
      "Epoch 127/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103091.1562 - val_loss: 192653.8438\n",
      "Epoch 128/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 100984.8984 - val_loss: 136247.9844\n",
      "Epoch 129/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 104726.7578 - val_loss: 214448.7500\n",
      "Epoch 130/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 102944.7656 - val_loss: 220474.2969\n",
      "Epoch 131/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 102313.3750 - val_loss: 167956.6719\n",
      "Epoch 132/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 99796.8047 - val_loss: 159371.2188\n",
      "Epoch 133/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 98037.6797 - val_loss: 139035.0781\n",
      "Epoch 134/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 98713.0547 - val_loss: 171375.2188\n",
      "Epoch 135/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 97669.2656 - val_loss: 195528.9219\n",
      "Epoch 136/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 98678.0781 - val_loss: 225601.6875\n",
      "Epoch 137/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 98754.0391 - val_loss: 171520.7500\n",
      "Epoch 138/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 98575.6953 - val_loss: 133793.3281\n",
      "Epoch 139/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 102322.4219 - val_loss: 147108.5469\n",
      "Epoch 140/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 100485.0625 - val_loss: 155905.5000\n",
      "Epoch 141/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 99864.0938 - val_loss: 196279.6406\n",
      "Epoch 142/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 96711.9375 - val_loss: 222593.5469\n",
      "Epoch 143/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 103483.7969 - val_loss: 164819.5781\n",
      "Epoch 144/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 98627.8750 - val_loss: 138150.7188\n",
      "Epoch 145/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 101758.4141 - val_loss: 129924.2734\n",
      "Epoch 146/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 96727.5703 - val_loss: 199689.8125\n",
      "Epoch 147/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 96696.0312 - val_loss: 185894.6094\n",
      "Epoch 148/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 94560.7656 - val_loss: 138768.9531\n",
      "Epoch 149/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 94886.6094 - val_loss: 127731.2266\n",
      "Epoch 150/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 94878.2188 - val_loss: 158069.5625\n",
      "Epoch 151/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 94652.6719 - val_loss: 198960.2812\n",
      "Epoch 152/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 99008.5625 - val_loss: 183315.5938\n",
      "Epoch 153/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 96583.7656 - val_loss: 159999.5781\n",
      "Epoch 154/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 96827.0703 - val_loss: 129726.6250\n",
      "Epoch 155/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 99812.1562 - val_loss: 198975.7031\n",
      "Epoch 156/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 94939.8125 - val_loss: 173773.3594\n",
      "Epoch 157/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 92801.3281 - val_loss: 143614.5469\n",
      "Epoch 158/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 91057.5234 - val_loss: 125966.7031\n",
      "Epoch 159/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90873.1641 - val_loss: 135015.0625\n",
      "Epoch 160/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 93824.2891 - val_loss: 159635.4219\n",
      "Epoch 161/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 93661.6641 - val_loss: 153783.7031\n",
      "Epoch 162/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 92913.1172 - val_loss: 176258.6250\n",
      "Epoch 163/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 100587.3047 - val_loss: 156529.7812\n",
      "Epoch 164/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 91946.8516 - val_loss: 153252.4844\n",
      "Epoch 165/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90569.5938 - val_loss: 129972.4609\n",
      "Epoch 166/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90933.5625 - val_loss: 157191.5000\n",
      "Epoch 167/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 90428.4922 - val_loss: 155504.2031\n",
      "Epoch 168/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 92911.0391 - val_loss: 135247.6094\n",
      "Epoch 169/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 91433.5312 - val_loss: 135514.1094\n",
      "Epoch 170/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 90281.0000 - val_loss: 117394.0781\n",
      "Epoch 171/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 92533.5469 - val_loss: 135326.5781\n",
      "Epoch 172/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 97545.5781 - val_loss: 154205.3594\n",
      "Epoch 173/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 95312.7031 - val_loss: 173481.2031\n",
      "Epoch 174/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 90612.4766 - val_loss: 153475.5469\n",
      "Epoch 175/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 88705.3906 - val_loss: 133683.8750\n",
      "Epoch 176/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90000.7188 - val_loss: 114878.4609\n",
      "Epoch 177/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 90879.4922 - val_loss: 125976.3594\n",
      "Epoch 178/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 91723.8828 - val_loss: 145905.9219\n",
      "Epoch 179/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 90145.7812 - val_loss: 132252.7344\n",
      "Epoch 180/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 97741.6719 - val_loss: 187644.3906\n",
      "Epoch 181/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 96200.5625 - val_loss: 131688.8438\n",
      "Epoch 182/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 91747.0234 - val_loss: 115899.1250\n",
      "Epoch 183/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 89088.7969 - val_loss: 138880.2344\n",
      "Epoch 184/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 87006.4609 - val_loss: 148981.7500\n",
      "Epoch 185/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 88791.5391 - val_loss: 117559.3906\n",
      "Epoch 186/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 89796.8750 - val_loss: 119633.9688\n",
      "Epoch 187/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 91255.0938 - val_loss: 122019.2969\n",
      "Epoch 188/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 91874.0078 - val_loss: 177525.8281\n",
      "Epoch 189/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 92166.2422 - val_loss: 158758.4688\n",
      "Epoch 190/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 93811.6094 - val_loss: 129725.1016\n",
      "Epoch 191/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 88621.3359 - val_loss: 118443.1250\n",
      "Epoch 192/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86968.7969 - val_loss: 116751.7422\n",
      "Epoch 193/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 87222.5547 - val_loss: 133256.9062\n",
      "Epoch 194/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 85093.6250 - val_loss: 142310.9688\n",
      "Epoch 195/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 89483.4688 - val_loss: 137589.3750\n",
      "Epoch 196/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 88486.2500 - val_loss: 134610.7500\n",
      "Epoch 197/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86031.0703 - val_loss: 144675.7344\n",
      "Epoch 198/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 87761.7109 - val_loss: 148594.4531\n",
      "Epoch 199/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 86241.5078 - val_loss: 115152.1016\n",
      "Epoch 200/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90644.7578 - val_loss: 127068.1406\n",
      "Epoch 201/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 89166.3125 - val_loss: 127320.1875\n",
      "Epoch 202/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 89584.2109 - val_loss: 130193.6719\n",
      "Epoch 203/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 87050.7188 - val_loss: 124144.4688\n",
      "Epoch 204/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86124.7578 - val_loss: 133476.9531\n",
      "Epoch 205/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 90315.1094 - val_loss: 130305.4688\n",
      "Epoch 206/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86180.8828 - val_loss: 113854.4922\n",
      "Epoch 207/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86823.1797 - val_loss: 150018.4062\n",
      "Epoch 208/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 86114.7188 - val_loss: 124164.0156\n",
      "Epoch 209/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 84519.2656 - val_loss: 124224.1562\n",
      "Epoch 210/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 83057.1172 - val_loss: 114653.4609\n",
      "Epoch 211/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 83847.2891 - val_loss: 119795.3359\n",
      "Epoch 212/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86361.3438 - val_loss: 119547.0391\n",
      "Epoch 213/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 83805.8672 - val_loss: 119566.9766\n",
      "Epoch 214/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 84272.0859 - val_loss: 119165.4688\n",
      "Epoch 215/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 85960.1484 - val_loss: 131803.8125\n",
      "Epoch 216/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 84220.8984 - val_loss: 115448.1641\n",
      "Epoch 217/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 82983.4531 - val_loss: 157321.2031\n",
      "Epoch 218/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 86496.7812 - val_loss: 125056.6250\n",
      "Epoch 219/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 84460.9766 - val_loss: 134223.6875\n",
      "Epoch 220/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 81869.6484 - val_loss: 129406.3438\n",
      "Epoch 221/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 82056.2812 - val_loss: 115195.1250\n",
      "Epoch 222/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 81966.5859 - val_loss: 124145.7578\n",
      "Epoch 223/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 86151.0938 - val_loss: 117972.7891\n",
      "Epoch 224/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 86437.5078 - val_loss: 140010.4375\n",
      "Epoch 225/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 85845.9766 - val_loss: 118517.4453\n",
      "Epoch 226/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 82211.3984 - val_loss: 118619.3516\n",
      "Epoch 227/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 80116.5156 - val_loss: 120111.7031\n",
      "Epoch 228/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 80092.6016 - val_loss: 119628.9609\n",
      "Epoch 229/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 80792.6719 - val_loss: 106188.7109\n",
      "Epoch 230/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 80657.2500 - val_loss: 123917.8125\n",
      "Epoch 231/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 83270.5078 - val_loss: 125224.7266\n",
      "Epoch 232/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 86250.9297 - val_loss: 107089.5781\n",
      "Epoch 233/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 81832.8047 - val_loss: 132302.4688\n",
      "Epoch 234/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 85333.3750 - val_loss: 110693.1172\n",
      "Epoch 235/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 84068.9766 - val_loss: 106982.0703\n",
      "Epoch 236/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79661.7812 - val_loss: 112013.4219\n",
      "Epoch 237/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 79125.0781 - val_loss: 108759.3359\n",
      "Epoch 238/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 80941.8672 - val_loss: 109157.8281\n",
      "Epoch 239/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 82084.9922 - val_loss: 148901.9062\n",
      "Epoch 240/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 88573.4766 - val_loss: 134819.8750\n",
      "Epoch 241/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 80980.0781 - val_loss: 123119.0859\n",
      "Epoch 242/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78904.2812 - val_loss: 122948.1562\n",
      "Epoch 243/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78048.2422 - val_loss: 115731.6719\n",
      "Epoch 244/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78364.9766 - val_loss: 122329.4609\n",
      "Epoch 245/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 79786.1719 - val_loss: 103731.6250\n",
      "Epoch 246/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 80573.9609 - val_loss: 110771.3359\n",
      "Epoch 247/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 81118.7344 - val_loss: 154316.2188\n",
      "Epoch 248/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 82923.6250 - val_loss: 145967.9219\n",
      "Epoch 249/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78730.7344 - val_loss: 114017.4453\n",
      "Epoch 250/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 83130.6797 - val_loss: 113308.6016\n",
      "Epoch 251/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 82887.6094 - val_loss: 109080.1172\n",
      "Epoch 252/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79053.1562 - val_loss: 105822.1172\n",
      "Epoch 253/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79877.4688 - val_loss: 104318.2109\n",
      "Epoch 254/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79098.8984 - val_loss: 113417.9062\n",
      "Epoch 255/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 81079.7188 - val_loss: 109760.6406\n",
      "Epoch 256/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 82331.0703 - val_loss: 122504.6250\n",
      "Epoch 257/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 78175.6172 - val_loss: 126187.1562\n",
      "Epoch 258/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78333.9766 - val_loss: 121750.5000\n",
      "Epoch 259/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76579.5859 - val_loss: 118503.2891\n",
      "Epoch 260/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 77040.0703 - val_loss: 114099.0938\n",
      "Epoch 261/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76469.6016 - val_loss: 115118.7109\n",
      "Epoch 262/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 77486.1016 - val_loss: 144831.6562\n",
      "Epoch 263/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 80071.1484 - val_loss: 119281.0625\n",
      "Epoch 264/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 81369.2031 - val_loss: 109469.0156\n",
      "Epoch 265/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78844.0703 - val_loss: 115893.7578\n",
      "Epoch 266/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 78711.5469 - val_loss: 109420.2734\n",
      "Epoch 267/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 77083.0078 - val_loss: 123447.3750\n",
      "Epoch 268/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 79660.7734 - val_loss: 117449.9609\n",
      "Epoch 269/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 77136.2031 - val_loss: 109199.8750\n",
      "Epoch 270/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 78588.5625 - val_loss: 106290.9609\n",
      "Epoch 271/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76211.5391 - val_loss: 109866.0547\n",
      "Epoch 272/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76373.3438 - val_loss: 103406.1094\n",
      "Epoch 273/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 77660.2969 - val_loss: 118185.1328\n",
      "Epoch 274/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79372.3125 - val_loss: 136254.5469\n",
      "Epoch 275/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 77699.2188 - val_loss: 112295.4062\n",
      "Epoch 276/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75759.5938 - val_loss: 130639.7578\n",
      "Epoch 277/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76269.7422 - val_loss: 107917.4141\n",
      "Epoch 278/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76974.1016 - val_loss: 105575.3125\n",
      "Epoch 279/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76529.2656 - val_loss: 105782.7422\n",
      "Epoch 280/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 77012.6406 - val_loss: 104655.2344\n",
      "Epoch 281/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76076.7109 - val_loss: 108137.4766\n",
      "Epoch 282/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76343.4688 - val_loss: 109924.1016\n",
      "Epoch 283/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78214.7109 - val_loss: 113180.8281\n",
      "Epoch 284/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 79361.0859 - val_loss: 121309.9141\n",
      "Epoch 285/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 78320.3281 - val_loss: 119431.0625\n",
      "Epoch 286/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 79622.3125 - val_loss: 135269.0312\n",
      "Epoch 287/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 75656.6641 - val_loss: 131006.2109\n",
      "Epoch 288/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 76008.0625 - val_loss: 106139.2109\n",
      "Epoch 289/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74886.9297 - val_loss: 106010.6250\n",
      "Epoch 290/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76816.4531 - val_loss: 106330.2422\n",
      "Epoch 291/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75236.4297 - val_loss: 98477.8281\n",
      "Epoch 292/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76433.6875 - val_loss: 108248.7422\n",
      "Epoch 293/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76861.1250 - val_loss: 132440.0938\n",
      "Epoch 294/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 77263.9453 - val_loss: 104593.6875\n",
      "Epoch 295/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76067.2812 - val_loss: 122296.9141\n",
      "Epoch 296/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74057.6562 - val_loss: 103541.8359\n",
      "Epoch 297/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73817.4297 - val_loss: 98825.4609\n",
      "Epoch 298/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73538.2656 - val_loss: 100929.2812\n",
      "Epoch 299/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75274.2500 - val_loss: 110733.6406\n",
      "Epoch 300/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75872.1797 - val_loss: 103752.8906\n",
      "Epoch 301/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75766.1641 - val_loss: 107369.2109\n",
      "Epoch 302/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76439.8828 - val_loss: 108414.4297\n",
      "Epoch 303/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 75406.0234 - val_loss: 98345.6797\n",
      "Epoch 304/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 77907.1797 - val_loss: 103613.5156\n",
      "Epoch 305/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75101.5312 - val_loss: 104455.7734\n",
      "Epoch 306/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73272.0625 - val_loss: 97599.6641\n",
      "Epoch 307/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72990.5625 - val_loss: 100021.8125\n",
      "Epoch 308/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 72650.9297 - val_loss: 100186.8438\n",
      "Epoch 309/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72945.5391 - val_loss: 99655.8672\n",
      "Epoch 310/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73096.8203 - val_loss: 98496.2266\n",
      "Epoch 311/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73332.9922 - val_loss: 103912.0625\n",
      "Epoch 312/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76053.0078 - val_loss: 106535.8516\n",
      "Epoch 313/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 75820.2500 - val_loss: 105106.5000\n",
      "Epoch 314/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73389.4453 - val_loss: 107400.6484\n",
      "Epoch 315/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75320.5000 - val_loss: 103965.5469\n",
      "Epoch 316/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 75106.0703 - val_loss: 100391.0781\n",
      "Epoch 317/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 74437.4922 - val_loss: 97475.0938\n",
      "Epoch 318/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73048.1328 - val_loss: 100910.3281\n",
      "Epoch 319/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74813.9609 - val_loss: 118631.6719\n",
      "Epoch 320/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72863.1562 - val_loss: 98840.7266\n",
      "Epoch 321/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73048.6875 - val_loss: 103784.8125\n",
      "Epoch 322/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 71928.1250 - val_loss: 104852.9141\n",
      "Epoch 323/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70659.6328 - val_loss: 117097.5547\n",
      "Epoch 324/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 72697.6719 - val_loss: 98672.1875\n",
      "Epoch 325/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 72868.4766 - val_loss: 101947.9062\n",
      "Epoch 326/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74173.2578 - val_loss: 122259.2734\n",
      "Epoch 327/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 75237.3906 - val_loss: 100686.0547\n",
      "Epoch 328/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73456.2422 - val_loss: 112822.6797\n",
      "Epoch 329/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 72740.9453 - val_loss: 135615.6406\n",
      "Epoch 330/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 75153.6562 - val_loss: 107631.8594\n",
      "Epoch 331/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73600.1094 - val_loss: 97230.1641\n",
      "Epoch 332/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72578.2891 - val_loss: 118046.3359\n",
      "Epoch 333/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 71293.2969 - val_loss: 110789.7578\n",
      "Epoch 334/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73581.9766 - val_loss: 98463.5625\n",
      "Epoch 335/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73715.1953 - val_loss: 115991.8750\n",
      "Epoch 336/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73340.9219 - val_loss: 101390.8281\n",
      "Epoch 337/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70991.0547 - val_loss: 97324.7500\n",
      "Epoch 338/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69790.3047 - val_loss: 97403.4297\n",
      "Epoch 339/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69950.8594 - val_loss: 110022.2891\n",
      "Epoch 340/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 70743.1172 - val_loss: 105441.2031\n",
      "Epoch 341/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70849.0781 - val_loss: 94973.4297\n",
      "Epoch 342/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70703.6562 - val_loss: 94900.9141\n",
      "Epoch 343/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70459.9922 - val_loss: 103649.2500\n",
      "Epoch 344/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 74659.3672 - val_loss: 105894.1328\n",
      "Epoch 345/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 81369.6172 - val_loss: 125649.6562\n",
      "Epoch 346/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 76208.6172 - val_loss: 97597.7031\n",
      "Epoch 347/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71934.1328 - val_loss: 97779.8828\n",
      "Epoch 348/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69026.4531 - val_loss: 102492.8438\n",
      "Epoch 349/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68484.7734 - val_loss: 97009.4688\n",
      "Epoch 350/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67886.1250 - val_loss: 97085.3750\n",
      "Epoch 351/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68085.2734 - val_loss: 96670.5391\n",
      "Epoch 352/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67773.9219 - val_loss: 96330.3750\n",
      "Epoch 353/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68817.1953 - val_loss: 106905.9375\n",
      "Epoch 354/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72127.9922 - val_loss: 107112.7812\n",
      "Epoch 355/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 71175.3906 - val_loss: 109265.1797\n",
      "Epoch 356/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 78343.2500 - val_loss: 133139.3750\n",
      "Epoch 357/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73844.8438 - val_loss: 125196.6875\n",
      "Epoch 358/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70528.9297 - val_loss: 96121.1250\n",
      "Epoch 359/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70396.4844 - val_loss: 93704.4922\n",
      "Epoch 360/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68717.8906 - val_loss: 95119.8516\n",
      "Epoch 361/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68507.6641 - val_loss: 109779.3906\n",
      "Epoch 362/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68456.5781 - val_loss: 99663.4609\n",
      "Epoch 363/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69910.5234 - val_loss: 106617.5859\n",
      "Epoch 364/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70818.0312 - val_loss: 109000.8516\n",
      "Epoch 365/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71075.0312 - val_loss: 95761.6719\n",
      "Epoch 366/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70584.9453 - val_loss: 114252.2422\n",
      "Epoch 367/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68866.8047 - val_loss: 112416.8203\n",
      "Epoch 368/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71464.8125 - val_loss: 98733.7578\n",
      "Epoch 369/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68102.9922 - val_loss: 103027.4297\n",
      "Epoch 370/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68869.7344 - val_loss: 101925.4062\n",
      "Epoch 371/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68579.7031 - val_loss: 97572.4922\n",
      "Epoch 372/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69144.3359 - val_loss: 96624.7734\n",
      "Epoch 373/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69895.5312 - val_loss: 102890.1016\n",
      "Epoch 374/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72914.6641 - val_loss: 116053.0000\n",
      "Epoch 375/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73227.5312 - val_loss: 100850.5391\n",
      "Epoch 376/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72477.3906 - val_loss: 101727.9609\n",
      "Epoch 377/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69716.9531 - val_loss: 108116.4297\n",
      "Epoch 378/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69145.2969 - val_loss: 113757.6641\n",
      "Epoch 379/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70036.8984 - val_loss: 98364.5391\n",
      "Epoch 380/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 72298.5000 - val_loss: 102327.9453\n",
      "Epoch 381/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 70002.6328 - val_loss: 124385.5703\n",
      "Epoch 382/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71268.5938 - val_loss: 114118.4609\n",
      "Epoch 383/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70662.7812 - val_loss: 95072.0000\n",
      "Epoch 384/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69140.3594 - val_loss: 107892.5000\n",
      "Epoch 385/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67530.2031 - val_loss: 106637.5234\n",
      "Epoch 386/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66909.9297 - val_loss: 95259.4922\n",
      "Epoch 387/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67332.6328 - val_loss: 100131.2109\n",
      "Epoch 388/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66472.1172 - val_loss: 98425.9453\n",
      "Epoch 389/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67794.4688 - val_loss: 94508.1719\n",
      "Epoch 390/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68033.1172 - val_loss: 100881.2031\n",
      "Epoch 391/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69461.7109 - val_loss: 119426.1016\n",
      "Epoch 392/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 71098.9062 - val_loss: 103005.7656\n",
      "Epoch 393/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 73557.4922 - val_loss: 125228.3750\n",
      "Epoch 394/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70082.5078 - val_loss: 119438.7891\n",
      "Epoch 395/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68319.9609 - val_loss: 96371.8438\n",
      "Epoch 396/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66562.5547 - val_loss: 101171.2266\n",
      "Epoch 397/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66867.3359 - val_loss: 99941.9844\n",
      "Epoch 398/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65772.6797 - val_loss: 92947.6641\n",
      "Epoch 399/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66088.5156 - val_loss: 94105.8281\n",
      "Epoch 400/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65747.1719 - val_loss: 97517.3516\n",
      "Epoch 401/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68691.2656 - val_loss: 100589.7188\n",
      "Epoch 402/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69082.1406 - val_loss: 101208.4844\n",
      "Epoch 403/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68346.0156 - val_loss: 108906.9453\n",
      "Epoch 404/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69424.3672 - val_loss: 100596.8281\n",
      "Epoch 405/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67438.7422 - val_loss: 99245.5391\n",
      "Epoch 406/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67792.2891 - val_loss: 96207.3828\n",
      "Epoch 407/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66903.6328 - val_loss: 102512.7109\n",
      "Epoch 408/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68834.1172 - val_loss: 130973.8594\n",
      "Epoch 409/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 71161.6484 - val_loss: 118654.0000\n",
      "Epoch 410/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 70177.9375 - val_loss: 97297.2109\n",
      "Epoch 411/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69024.3672 - val_loss: 100125.0000\n",
      "Epoch 412/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67447.1016 - val_loss: 101656.8203\n",
      "Epoch 413/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65618.6641 - val_loss: 108620.4609\n",
      "Epoch 414/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65060.8125 - val_loss: 108132.3828\n",
      "Epoch 415/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66102.7266 - val_loss: 91452.7578\n",
      "Epoch 416/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64664.5781 - val_loss: 94535.4844\n",
      "Epoch 417/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64470.6055 - val_loss: 95724.1641\n",
      "Epoch 418/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64703.6133 - val_loss: 103075.8125\n",
      "Epoch 419/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67707.9766 - val_loss: 99958.5781\n",
      "Epoch 420/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68004.2500 - val_loss: 108863.9844\n",
      "Epoch 421/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68618.1016 - val_loss: 131393.3750\n",
      "Epoch 422/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69650.3516 - val_loss: 93431.8828\n",
      "Epoch 423/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67646.1094 - val_loss: 98759.8438\n",
      "Epoch 424/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65593.8516 - val_loss: 102743.6641\n",
      "Epoch 425/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67660.9062 - val_loss: 103566.2266\n",
      "Epoch 426/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 68101.6016 - val_loss: 92526.3047\n",
      "Epoch 427/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64848.9062 - val_loss: 93721.3750\n",
      "Epoch 428/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65904.6875 - val_loss: 101500.8047\n",
      "Epoch 429/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65114.6172 - val_loss: 100484.4453\n",
      "Epoch 430/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64390.7227 - val_loss: 93305.9609\n",
      "Epoch 431/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64747.2500 - val_loss: 94737.3047\n",
      "Epoch 432/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64677.7695 - val_loss: 100053.5000\n",
      "Epoch 433/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 70708.9453 - val_loss: 108545.8281\n",
      "Epoch 434/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69219.1094 - val_loss: 116249.7266\n",
      "Epoch 435/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66105.2734 - val_loss: 90556.3594\n",
      "Epoch 436/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65693.1875 - val_loss: 114527.4141\n",
      "Epoch 437/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65034.5234 - val_loss: 114198.9766\n",
      "Epoch 438/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65263.7383 - val_loss: 107968.2891\n",
      "Epoch 439/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67261.4609 - val_loss: 94009.3359\n",
      "Epoch 440/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 67366.2031 - val_loss: 103316.6250\n",
      "Epoch 441/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68564.3125 - val_loss: 94277.2578\n",
      "Epoch 442/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66907.6562 - val_loss: 101398.7266\n",
      "Epoch 443/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 69015.5234 - val_loss: 97827.9375\n",
      "Epoch 444/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66467.7656 - val_loss: 89665.9688\n",
      "Epoch 445/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64637.5625 - val_loss: 90185.0234\n",
      "Epoch 446/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64257.0000 - val_loss: 90933.3125\n",
      "Epoch 447/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63813.6641 - val_loss: 89347.4609\n",
      "Epoch 448/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64816.8477 - val_loss: 99666.3750\n",
      "Epoch 449/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66767.5234 - val_loss: 97594.0703\n",
      "Epoch 450/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66384.1719 - val_loss: 91103.8750\n",
      "Epoch 451/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66467.3359 - val_loss: 111110.0000\n",
      "Epoch 452/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66783.9844 - val_loss: 91708.0234\n",
      "Epoch 453/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64562.6172 - val_loss: 91674.5781\n",
      "Epoch 454/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65470.0391 - val_loss: 91404.2266\n",
      "Epoch 455/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64264.9688 - val_loss: 89522.0625\n",
      "Epoch 456/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64238.0977 - val_loss: 89275.3594\n",
      "Epoch 457/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64692.6523 - val_loss: 88035.8203\n",
      "Epoch 458/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64746.0312 - val_loss: 93228.4609\n",
      "Epoch 459/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67016.9531 - val_loss: 97845.4531\n",
      "Epoch 460/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 69923.1094 - val_loss: 91578.1719\n",
      "Epoch 461/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66967.0312 - val_loss: 101251.7109\n",
      "Epoch 462/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 67235.4688 - val_loss: 93408.6797\n",
      "Epoch 463/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 65309.1523 - val_loss: 92300.8906\n",
      "Epoch 464/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 64124.6055 - val_loss: 94349.6016\n",
      "Epoch 465/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63517.1523 - val_loss: 93724.8984\n",
      "Epoch 466/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63315.7695 - val_loss: 89215.8359\n",
      "Epoch 467/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62340.8672 - val_loss: 93985.1016\n",
      "Epoch 468/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62772.9219 - val_loss: 90720.5859\n",
      "Epoch 469/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62945.3750 - val_loss: 89513.5781\n",
      "Epoch 470/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 63638.0000 - val_loss: 89455.2031\n",
      "Epoch 471/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65675.0391 - val_loss: 107076.1094\n",
      "Epoch 472/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 73506.8203 - val_loss: 97669.0547\n",
      "Epoch 473/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 66598.8906 - val_loss: 99679.9375\n",
      "Epoch 474/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65470.5898 - val_loss: 104499.5156\n",
      "Epoch 475/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64629.0742 - val_loss: 95860.7109\n",
      "Epoch 476/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63492.9648 - val_loss: 90843.4062\n",
      "Epoch 477/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64470.5039 - val_loss: 97450.7422\n",
      "Epoch 478/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65502.2070 - val_loss: 97376.3984\n",
      "Epoch 479/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 68431.9688 - val_loss: 101174.3438\n",
      "Epoch 480/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66032.0859 - val_loss: 99791.2500\n",
      "Epoch 481/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 64779.5156 - val_loss: 105923.0703\n",
      "Epoch 482/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62721.0586 - val_loss: 89961.5859\n",
      "Epoch 483/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 61934.4141 - val_loss: 92388.9531\n",
      "Epoch 484/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 61531.4180 - val_loss: 91749.4453\n",
      "Epoch 485/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62647.3242 - val_loss: 111588.0547\n",
      "Epoch 486/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 63069.3359 - val_loss: 90982.2109\n",
      "Epoch 487/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62255.2344 - val_loss: 93615.9219\n",
      "Epoch 488/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62019.3438 - val_loss: 94774.9219\n",
      "Epoch 489/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62332.3203 - val_loss: 92002.6094\n",
      "Epoch 490/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63775.7500 - val_loss: 92983.1484\n",
      "Epoch 491/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65466.6875 - val_loss: 97347.5781\n",
      "Epoch 492/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63630.6836 - val_loss: 94222.1016\n",
      "Epoch 493/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 63674.4297 - val_loss: 93193.7422\n",
      "Epoch 494/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63335.7383 - val_loss: 98914.5859\n",
      "Epoch 495/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62998.6875 - val_loss: 99005.2109\n",
      "Epoch 496/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 62652.0664 - val_loss: 99870.1250\n",
      "Epoch 497/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 63488.3906 - val_loss: 107990.7891\n",
      "Epoch 498/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62917.1484 - val_loss: 102266.3984\n",
      "Epoch 499/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 63034.3281 - val_loss: 94897.5078\n",
      "Epoch 500/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 62041.3320 - val_loss: 94525.6250\n",
      "Epoch 501/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 61599.0352 - val_loss: 99878.4844\n",
      "Epoch 502/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 61920.4766 - val_loss: 88443.6406\n",
      "Epoch 503/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 63192.9648 - val_loss: 103741.7578\n",
      "Epoch 504/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66268.8516 - val_loss: 114028.5078\n",
      "Epoch 505/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 66415.6016 - val_loss: 113295.0859\n",
      "Epoch 506/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 70215.4375 - val_loss: 103084.1484\n",
      "Epoch 507/1000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 65433.3789 - val_loss: 97627.7812\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### # Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the model and get the training history with early stopping\n",
    "history = hybrid_sr_model.fit(\n",
    "    X_train_lr, \n",
    "    X_train_hr, \n",
    "    epochs=1000, \n",
    "    batch_size=4, \n",
    "    validation_data=(X_validation_lr, X_validation_hr),\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")\n",
    "\n",
    "# Visualize training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f6bef1-abb5-4b52-bf37-31525ce1da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Average PSNR on the test set: 29.559025148550667\n",
      "Average SSIM on the test set: 0.815258\n",
      "Average SAM on the test set (in degrees): 4.20522837741388\n",
      "Average Correlation Coefficient on the test set: 0.9239289062356134\n",
      "Average ERGAS on the test set: 3.0164642496453586\n",
      "Average RMSE: 0.03415633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def psnr(y_true, y_pred, max_pixel=None):\n",
    "    \"\"\"\n",
    "    Compute PSNR for each spectral band separately and return the average.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth image, shape (H, W, B)\n",
    "        y_pred: Super-resolved image, shape (H, W, B)\n",
    "        max_pixel: Maximum pixel value (None = use actual max from y_true)\n",
    "    \n",
    "    Returns:\n",
    "        Average PSNR across all bands\n",
    "    \"\"\"\n",
    "    if max_pixel is None:\n",
    "        max_pixel = np.max(y_true)  # Auto-detect max value if not provided\n",
    "\n",
    "    B = y_true.shape[-1]  # Number of spectral bands\n",
    "    psnr_values = []\n",
    "    \n",
    "    for i in range(B):  # Loop over bands\n",
    "        mse = np.mean((y_true[..., i] - y_pred[..., i]) ** 2)\n",
    "        if mse == 0:\n",
    "            psnr_values.append(float('inf'))  # Perfect reconstruction\n",
    "        else:\n",
    "            psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "            psnr_values.append(psnr)\n",
    "    \n",
    "    return np.mean(psnr_values)  # Average across bands\n",
    "\n",
    "# Function to calculate SSIM with channel_axis\n",
    "def ssim_value(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape}\")\n",
    "    \n",
    "    data_range = y_true.max() - y_true.min()  # Calculate data range from y_true\n",
    "    ssim_val = ssim(y_true, y_pred, data_range=data_range, channel_axis=-1)\n",
    "    return ssim_val\n",
    "\n",
    "# Function to calculate Correlation Coefficient\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    corr_matrix = np.corrcoef(y_true_flat, y_pred_flat)\n",
    "    corr_value = corr_matrix[0, 1]\n",
    "    return corr_value\n",
    "\n",
    "# Function to calculate Spectral Angle Mapper (SAM) in degrees\n",
    "def sam(y_true, y_pred):\n",
    "    y_true_reshaped = y_true.reshape(-1, y_true.shape[-1])\n",
    "    y_pred_reshaped = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    non_zero_mask = (np.linalg.norm(y_true_reshaped, axis=1) > 1e-10) & (np.linalg.norm(y_pred_reshaped, axis=1) > 1e-10)\n",
    "    dot_product = np.sum(y_true_reshaped[non_zero_mask] * y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    norm_true = np.linalg.norm(y_true_reshaped[non_zero_mask], axis=1)\n",
    "    norm_pred = np.linalg.norm(y_pred_reshaped[non_zero_mask], axis=1)\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        angles = np.arccos(np.clip(dot_product / (norm_true * norm_pred), -1.0, 1.0))\n",
    "    \n",
    "    if angles.size > 0:\n",
    "        sam_value_degrees = np.mean(angles) * (180 / np.pi)\n",
    "    else:\n",
    "        sam_value_degrees = 0\n",
    "    \n",
    "    return sam_value_degrees\n",
    "\n",
    "# Function to normalize the images\n",
    "def normalize(image):\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "\n",
    "# Function to calculate Root Mean Squared Error (RMSE) for hyperspectral images (normalized)\n",
    "def rmse_bandwise(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch between true and predicted images.\")\n",
    "    \n",
    "    bands = y_true.shape[-1]\n",
    "    rmse_per_band = []\n",
    "\n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        \n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)\n",
    "        rmse_band_value = np.sqrt(mse_band)\n",
    "        rmse_per_band.append(rmse_band_value)\n",
    "\n",
    "    # Normalize RMSE by the maximum value in y_true across all bands\n",
    "    max_value = np.max(y_true)\n",
    "    normalized_rmse = np.mean(rmse_per_band) / max_value\n",
    "    return normalized_rmse\n",
    "\n",
    "# Function to calculate ERGAS\n",
    "def ergas(y_true, y_pred, scale):\n",
    "    bands = y_true.shape[-1]\n",
    "    ergas_value = 0\n",
    "    \n",
    "    for b in range(bands):\n",
    "        band_true = y_true[:, :, b]\n",
    "        band_pred = y_pred[:, :, b]\n",
    "        mean_band_true = np.mean(band_true)\n",
    "        \n",
    "        # Calculate RMSE for the band without using a separate function\n",
    "        mse_band = np.mean((band_true - band_pred) ** 2)  # Mean Squared Error for the band\n",
    "        rmse_band = np.sqrt(mse_band)  # Root Mean Squared Error for the band\n",
    "        \n",
    "        ergas_value += (rmse_band / mean_band_true) ** 2\n",
    "    \n",
    "    ergas_value = 100 * (1 / scale) * np.sqrt(ergas_value / bands)\n",
    "    return ergas_value\n",
    "\n",
    "# Assuming hybrid_sr_model is trained, and X_test_lr, X_test_hr are defined\n",
    "predicted_hr_images = hybrid_sr_model.predict(X_test_lr, batch_size=4)\n",
    "\n",
    "downscale_factor = 8 # ERGAS downscale factor\n",
    "\n",
    "# Validate shapes match for test and predictions\n",
    "if predicted_hr_images.shape != X_test_hr.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_hr_images shape {predicted_hr_images.shape} vs X_test_hr shape {X_test_hr.shape}\")\n",
    "\n",
    "# Calculate metrics per test sample\n",
    "psnr_values, ssim_values, cc_values, sam_values, ergas_values, rmse_values = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test_hr)):\n",
    "    psnr_values.append(psnr(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ssim_values.append(ssim_value(X_test_hr[i], predicted_hr_images[i]))\n",
    "    cc_values.append(correlation_coefficient(X_test_hr[i], predicted_hr_images[i]))\n",
    "    sam_values.append(sam(X_test_hr[i], predicted_hr_images[i]))\n",
    "    ergas_values.append(ergas(X_test_hr[i], predicted_hr_images[i], downscale_factor))\n",
    "    rmse_values.append(rmse_bandwise(X_test_hr[i], predicted_hr_images[i]))\n",
    "\n",
    "# Average metrics\n",
    "average_psnr = np.mean(psnr_values)\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_cc = np.mean(cc_values)\n",
    "average_sam = np.mean(sam_values)\n",
    "average_ergas = np.mean(ergas_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Average PSNR on the test set:\", average_psnr)\n",
    "print(\"Average SSIM on the test set:\", average_ssim)\n",
    "print(\"Average SAM on the test set (in degrees):\", average_sam)\n",
    "print(\"Average Correlation Coefficient on the test set:\", average_cc)\n",
    "print(\"Average ERGAS on the test set:\", average_ergas)\n",
    "print(\"Average RMSE:\", average_rmse)  # Indicate RMSE is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bfb77-74c1-4c34-825b-e309932acfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
